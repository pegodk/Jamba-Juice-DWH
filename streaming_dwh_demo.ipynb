{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Streaming Data Warehousing\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n",
    "\n",
    "## Bronze layer (raw data)\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "## Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.\n",
    "\n",
    "## Gold layer (curated business-level tables)\n",
    "Data in the Gold layer of the lakehouse is typically organized in consumption-ready \"project-specific\" databases. The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here. Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer. We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit in this Gold Layer of the lakehouse.\n",
    "\n",
    "So you can see that the data is curated as it moves through the different layers of a lakehouse. In some cases, we also see that lot of Data Marts and EDWs from the traditional RDBMS technology stack are ingested into the lakehouse, so that for the first time Enterprises can do \"pan-EDW\" advanced analytics and ML - which was just not possible or too cost prohibitive to do on a traditional stack. (e.g. IoT/Manufacturing data is tied with Sales and Marketing data for defect analysis or health care genomics, EMR/HL7 clinical data markets are tied with financial claims data to create a Healthcare Data Lake for timely and improved patient care analytics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Data Ingestion\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- meta_created: Timestamp from when the row was ingested\n",
    "- meta_filename: Filename from which the row was ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "def create_bronze_streaming_table(source_folder, target_table):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source_folder)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .trigger(processingTime='10 seconds')\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target_table}\")\n",
    "        .queryName(target_table)\n",
    "        .toTable(target_table)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "create_bronze_streaming_table(source_folder=\"data/inventory\", target_table=\"bronze_inventory\")\n",
    "create_bronze_streaming_table(source_folder=\"data/product\", target_table=\"bronze_product\")\n",
    "create_bronze_streaming_table(source_folder=\"data/sales\", target_table=\"bronze_sales\")\n",
    "create_bronze_streaming_table(source_folder=\"data/customer\", target_table=\"bronze_customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Slowly Changing Dimensions\n",
    "Type 1 Slowly Changing Dimension: This method overwrites the existing value with the new value and does not retain history. Type 2 Slowly Changing Dimension: This method adds a new row for the new value and maintains the existing row for historical and reporting purposes.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following meta columns will be created:\n",
    "\n",
    "SCD Type 1:\n",
    "- meta_hashdiff\n",
    "- meta_last_updated\n",
    "- meta_sequence\n",
    "\n",
    "SCD Type 2:\n",
    "- meta_hashdiff\n",
    "- meta_is_current\n",
    "- meta_valid_from\n",
    "- meta_valid_to\n",
    "- meta_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    # Run and print SQL query\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "\n",
    "create_silver_table_schema(table_name=\"silver_sales_scd1\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "create_silver_table_schema(table_name=\"silver_inventory_scd1\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "create_silver_table_schema(table_name=\"silver_product_scd2\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "create_silver_table_schema(table_name=\"silver_customer_scd2\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd1_table(df: DataFrame, id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    # TODO: Find a better way to pass input parameters to functions \n",
    "    target = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT *\n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_scd1_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_last_updated\", current_timestamp())\n",
    "        .withColumn(\"parameter_target\", lit(target_table))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd1_table)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_silver_scd1_streaming_table (\n",
    "    source_table=\"bronze_sales\",\n",
    "    target_table=\"silver_sales_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    join_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\"\n",
    ")\n",
    "\n",
    "create_silver_scd1_streaming_table (\n",
    "    source_table=\"bronze_inventory\",\n",
    "    target_table=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    join_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd2_table(df: DataFrame, batch_id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    target_table = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "\n",
    "    # Drop extra columns\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Reorder dataframe to have same order as target table (otherwise insert statement might fail)\n",
    "    df_target = spark.read.table(target_table).limit(1)\n",
    "    df = df.select(df_target.columns)\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target_table} AS t\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(merge_query)\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target_table}\n",
    "            SELECT s.*\n",
    "            FROM tempView s\n",
    "            JOIN {target_table} t ON t.{join_key} = s.{join_key}\n",
    "            WHERE s.meta_sequence = {seq_num}\n",
    "            AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_scd2_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "\n",
    "    # TODO: Find a better way to pass arguments to the foreachBatch function!!!\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "        .withColumn(\"parameter_target\", lit(target_table))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd2_table)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SCD2 tables\n",
    "create_silver_scd2_streaming_table(\n",
    "    source_table = \"bronze_product\",\n",
    "    target_table = \"silver_product_scd2\",\n",
    "    join_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\"\n",
    ")\n",
    "\n",
    "create_silver_scd2_streaming_table(\n",
    "    source_table = \"bronze_customer\",\n",
    "    target_table = \"silver_customer_scd2\",\n",
    "    join_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Facts and Dimensions\n",
    "## What is Dimensional Modeling\n",
    "The data model used to store data in the denormalized form is called Dimensional Modeling. It is the technique of storing data in a Data Warehouse in such a way that enables fast query performance and easy access to its business users. It involves creating a set of dimensional tables that are designed to support business intelligence and reporting needs.\n",
    "\n",
    "The core concept of dimensional modeling is the creation of a star schema. It is called so as the tables are arranged in the form of a star.\n",
    "\n",
    "![alt text](images/star_schema.png \"Star Schema\")\n",
    "\n",
    "Dimensional modeling includes facts and dimensions. Let’s have a basic idea of what Facts and Dimensions are.\n",
    "\n",
    "## Fact Tables\n",
    "Fact tables are the heart of a data warehouse. They contain quantitative data, often referred to as measures or metrics, and are the focus of most data analysis. These tables store data related to business transactions and events, such as sales figures, revenue, or quantities sold. In essence, fact tables provide the “what” in data analysis.\n",
    "\n",
    "## Dimension Tables\n",
    "Dimension tables, on the other hand, offer context to the data stored in fact tables. They provide descriptive information that helps users understand the “who,” “where,” and “when” aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict,\n",
    "        include_unknown_column : bool\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE OR REPLACE TABLE {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "    if include_unknown_column:\n",
    "        print(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n",
    "        spark.sql(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_product\",\n",
    "    source_table=\"silver_product_scd2\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_customer\",\n",
    "    source_table=\"silver_customer_scd2\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    include_unknown_column=False\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    include_unknown_column=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimension_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Print\n",
    "    print(\"\\nMerging into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    query = f\"SELECT * FROM {source_table}\"\n",
    "\n",
    "    # Add delta load logic if the target_table table already exists\n",
    "    if delta_load_column:\n",
    "        query += f\"\\n WHERE {delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target_table})\"\n",
    "\n",
    "    # Run query\n",
    "    df = spark.sql(query)\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dimension_table (\n",
    "    source_table=\"silver_product_scd2\",\n",
    "    target_table=\"gold_dim_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_dimension_table (\n",
    "    source_table=\"silver_customer_scd2\",\n",
    "    target_table=\"gold_dim_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dim_table_references(source, target, timestamp_key, dim_table_refs, delta_load_column, print_output=True):\n",
    "    \n",
    "    query_first = \"SELECT s.*\"\n",
    "    query_last = f\"\\nFROM {source} s\"\n",
    "\n",
    "    for ref in dim_table_refs:\n",
    "        \n",
    "        # Construct first part of query: Selects\n",
    "        query_first += f\"\"\", COALESCE({ref[\"table_name\"]}.{ref[\"surrogate_key\"]}, 'N/A') AS {ref[\"surrogate_key\"]} \"\"\"\n",
    "\n",
    "        # Construct last part of query: Joins\n",
    "        query_last += f\"\"\"\\nLEFT JOIN {ref[\"table_name\"]} ON {ref[\"table_name\"]}.{ref[\"join_key\"]} = s.{ref[\"join_key\"]}\n",
    "        AND s.{timestamp_key} BETWEEN {ref[\"table_name\"]}.meta_valid_from AND {ref[\"table_name\"]}.meta_valid_to\"\"\"\n",
    "\n",
    "    # Add delta load logic if the target table already exists\n",
    "    if delta_load_column:\n",
    "        query_last += f\"\\n WHERE s.{delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target})\"\n",
    "    \n",
    "    # Print output\n",
    "    if print_output:\n",
    "        print(query_first + query_last)\n",
    "\n",
    "    return query_first + query_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fact_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Print\n",
    "    print(\"\\nMerging into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source_table,\n",
    "                                                 target=target_table,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_fact_table (\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    target_table=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "process_fact_table (\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    target_table=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
