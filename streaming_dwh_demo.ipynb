{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Warehousing Demo\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n",
    "\n",
    "### Bronze layer (raw data)\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "### Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.\n",
    "\n",
    "### Gold layer (curated business-level tables)\n",
    "Data in the Gold layer of the lakehouse is typically organized in consumption-ready \"project-specific\" databases. The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here. Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer. We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit in this Gold Layer of the lakehouse.\n",
    "\n",
    "So you can see that the data is curated as it moves through the different layers of a lakehouse. In some cases, we also see that lot of Data Marts and EDWs from the traditional RDBMS technology stack are ingested into the lakehouse, so that for the first time Enterprises can do \"pan-EDW\" advanced analytics and ML - which was just not possible or too cost prohibitive to do on a traditional stack. (e.g. IoT/Manufacturing data is tied with Sales and Marketing data for defect analysis or health care genomics, EMR/HL7 clinical data markets are tied with financial claims data to create a Healthcare Data Lake for timely and improved patient care analytics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Data Ingestion\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- **meta_created**: Timestamp from when the row was ingested\n",
    "- **meta_filename**: Filename from which the row was ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f02c2d7d4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "def bronze_streaming_table(source_folder, target_table):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source_folder)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .trigger(processingTime='10 seconds')\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target_table}\")\n",
    "        .queryName(target_table)\n",
    "        .toTable(target_table)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "bronze_streaming_table(source_folder=\"data/inventory\", target_table=\"bronze_inventory\")\n",
    "bronze_streaming_table(source_folder=\"data/product\", target_table=\"bronze_product\")\n",
    "bronze_streaming_table(source_folder=\"data/sales\", target_table=\"bronze_sales\")\n",
    "bronze_streaming_table(source_folder=\"data/customer\", target_table=\"bronze_customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Slowly Changing Dimensions\n",
    "Type 1 Slowly Changing Dimension: This method overwrites the existing value with the new value and does not retain history. Type 2 Slowly Changing Dimension: This method adds a new row for the new value and maintains the existing row for historical and reporting purposes.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following meta columns will be created:\n",
    "\n",
    "### SCD Type 1:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns. [Read more here.](https://www.tpximpact.com/knowledge-hub/blogs/tech/hash-keys-data-warehousing-2/)\n",
    "- **meta_last_updated**: Timestamp of when the row was last updated/overwritten.\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key.\n",
    "\n",
    "### SCD Type 2:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns.\n",
    "- **meta_is_current**: Boolean of whether this row is the current (most recent).\n",
    "- **meta_valid_from**: Timestamp of when the row was first ingested.\n",
    "- **meta_valid_to**: Timestamp of then the row was outdated by a newer version (Null if row is current).\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS silver_sales (transaction_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_inventory (inventory_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_product (product_sid string, category string, cogs double, contains_caffeine boolean, contains_fruit boolean, contains_nuts boolean, contains_veggies boolean, event_time string, item string, price double, product_id string, size string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_customer (customer_sid string, address string, credit_card_expire string, credit_card_number string, customer_id bigint, email string, event_time string, full_name string, phone_number string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n"
     ]
    }
   ],
   "source": [
    "def silver_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    # Run and print SQL query\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "\n",
    "silver_table_schema(table_name=\"silver_sales\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_inventory\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_product\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "silver_table_schema(table_name=\"silver_customer\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 1 & 2\n",
    "\n",
    "In the Type 1 SCD, you simply overwrite data in dimensions. [Read more here.](https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def upsert_to_scd_table(\n",
    "        target_table: str,\n",
    "        timestamp_key: str,\n",
    "        join_key: str,\n",
    "        scd_type: int\n",
    "    ):\n",
    "\n",
    "    def _inner_func(df, batch_id):\n",
    "        \n",
    "        # Set default values for meta columns depending on SCD type\n",
    "        if scd_type == 1:\n",
    "            df  = df.withColumn(\"meta_last_updated\", current_timestamp())\n",
    "        elif scd_type == 2:\n",
    "            df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "            df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "            df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "        else:\n",
    "            raise ValueError(\"Currently only supports SCD type 1 and 2\")\n",
    "        \n",
    "        # Calculate hashdiff\n",
    "        df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "        # Calculate sequence number\n",
    "        df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "        # Reorder columns to match target table (only neccessary for SCD type 2)\n",
    "        if scd_type == 2:\n",
    "            df_target = spark.read.table(target_table).limit(1)\n",
    "            df = df.select(df_target.columns)\n",
    "\n",
    "        # Create view with source data\n",
    "        df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "        # Get list of sequences\n",
    "        lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "        # Loop over the sequence and do merge into statements for either SCD type 1 or 2\n",
    "        for seq_num in lst_sequence:\n",
    "            if scd_type == 1:\n",
    "                query = f\"\"\"\n",
    "                    MERGE INTO {target_table} AS t\n",
    "                    USING (\n",
    "                        SELECT *\n",
    "                        FROM tempView\n",
    "                        WHERE meta_sequence = {seq_num}\n",
    "                    ) AS s ON t.{join_key} = s.{join_key}\n",
    "                    WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                        THEN UPDATE SET *\n",
    "                    WHEN NOT MATCHED \n",
    "                        THEN INSERT *\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(query)\n",
    "\n",
    "            elif scd_type == 2:\n",
    "                merge_query = f\"\"\"\n",
    "                    MERGE INTO {target_table} AS t\n",
    "                    USING (\n",
    "                        SELECT * \n",
    "                        FROM tempView\n",
    "                        WHERE meta_sequence = {seq_num}\n",
    "                    ) AS s ON t.{join_key} = s.{join_key}\n",
    "                    WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                        THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "                    WHEN NOT MATCHED \n",
    "                        THEN INSERT *\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(merge_query)\n",
    "\n",
    "                insert_query = f\"\"\"\n",
    "                    INSERT INTO {target_table}\n",
    "                    SELECT s.*\n",
    "                    FROM tempView s\n",
    "                    JOIN {target_table} t ON t.{join_key} = s.{join_key}\n",
    "                    WHERE s.meta_sequence = {seq_num}\n",
    "                    AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(insert_query)\n",
    "    \n",
    "    return _inner_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str,\n",
    "    scd_type: int\n",
    "):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd_table(\n",
    "            target_table=target_table, \n",
    "            timestamp_key=timestamp_key, \n",
    "            join_key=join_key, \n",
    "            scd_type=scd_type\n",
    "        ))\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f02c2d4ded0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create silver tables with SCD type 1\n",
    "silver_streaming_table (\n",
    "    source_table=\"bronze_sales\",\n",
    "    target_table=\"silver_sales\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    join_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    scd_type=1\n",
    ")\n",
    "\n",
    "silver_streaming_table (\n",
    "    source_table=\"bronze_inventory\",\n",
    "    target_table=\"silver_inventory\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    join_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    scd_type=1\n",
    ")\n",
    "\n",
    "# Create silver tables with SCD type 2\n",
    "silver_streaming_table(\n",
    "    source_table = \"bronze_product\",\n",
    "    target_table = \"silver_product\",\n",
    "    join_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\",\n",
    "    scd_type=2\n",
    ")\n",
    "\n",
    "silver_streaming_table(\n",
    "    source_table = \"bronze_customer\",\n",
    "    target_table = \"silver_customer\",\n",
    "    join_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\",\n",
    "    scd_type=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Facts and Dimensions\n",
    "## What is Dimensional Modeling\n",
    "The data model used to store data in the denormalized form is called Dimensional Modeling. It is the technique of storing data in a Data Warehouse in such a way that enables fast query performance and easy access to its business users. It involves creating a set of dimensional tables that are designed to support business intelligence and reporting needs.\n",
    "\n",
    "The core concept of dimensional modeling is the creation of a star schema. It is called so as the tables are arranged in the form of a star.\n",
    "\n",
    "![alt text](images/star_schema.png \"Star Schema\")\n",
    "\n",
    "Dimensional modeling includes facts and dimensions. Let’s have a basic idea of what Facts and Dimensions are.\n",
    "\n",
    "## Fact Tables\n",
    "Fact tables are the heart of a data warehouse. They contain quantitative data, often referred to as measures or metrics, and are the focus of most data analysis. These tables store data related to business transactions and events, such as sales figures, revenue, or quantities sold. In essence, fact tables provide the “what” in data analysis.\n",
    "\n",
    "## Dimension Tables\n",
    "Dimension tables, on the other hand, offer context to the data stored in fact tables. They provide descriptive information that helps users understand the “who,” “where,” and “when” aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict,\n",
    "        include_unknown_column : bool\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE OR REPLACE TABLE {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "    if include_unknown_column:\n",
    "        print(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n",
    "        spark.sql(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE OR REPLACE TABLE gold_dim_product (product_sid string, category string, cogs double, contains_caffeine boolean, contains_fruit boolean, contains_nuts boolean, contains_veggies boolean, event_time string, item string, price double, product_id string, size string, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA;\n",
      "INSERT INTO gold_dim_product (product_sid) VALUES ('N/A')\n",
      "CREATE OR REPLACE TABLE gold_dim_customer (customer_sid string, address string, credit_card_expire string, credit_card_number string, customer_id bigint, email string, event_time string, full_name string, phone_number string, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA;\n",
      "INSERT INTO gold_dim_customer (customer_sid) VALUES ('N/A')\n",
      "CREATE OR REPLACE TABLE gold_fact_sales (transaction_sid string, product_sid string, customer_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n",
      "CREATE OR REPLACE TABLE gold_fact_inventory (inventory_sid string, product_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n"
     ]
    }
   ],
   "source": [
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_product\",\n",
    "    source_table=\"silver_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_customer\",\n",
    "    source_table=\"silver_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    include_unknown_column=False\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    include_unknown_column=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_func_utils import generate_dim_table_references\n",
    "\n",
    "def process_gold_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column : str = None,\n",
    "    timestamp_key : str = \"\",\n",
    "    dim_table_refs : list[dict] = []\n",
    "):\n",
    "    print(\"\\nProcessesing changes into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    if \"fact\" in target_table:\n",
    "        query = generate_dim_table_references(\n",
    "            source_table=source_table,\n",
    "            timestamp_key=timestamp_key, \n",
    "            dim_table_refs=dim_table_refs)\n",
    "    else:\n",
    "        query = f\"SELECT s.* FROM {source_table} s\"\n",
    "\n",
    "    # Add delta load logic if the target_table table already exists\n",
    "    if delta_load_column:\n",
    "        query += f\"\\n WHERE s.{delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target_table})\"\n",
    "\n",
    "    # Run query and create view\n",
    "    spark.sql(query).createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processesing changes into gold_dim_product\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "\n",
      "Processesing changes into gold_dim_customer\n"
     ]
    },
    {
     "ename": "UnsupportedOperationException",
     "evalue": "[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m process_gold_table (\n\u001b[1;32m      2\u001b[0m     source_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_product\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     target_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_dim_product\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     surrogate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_sid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     delta_load_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprocess_gold_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilver_customer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgold_dim_customer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_sid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta_load_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m process_gold_table (\n\u001b[1;32m     16\u001b[0m     source_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     target_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_fact_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m process_gold_table (\n\u001b[1;32m     28\u001b[0m     source_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_inventory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     target_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_fact_inventory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     dim_table_refs\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_dim_product\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogate_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_sid\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m     34\u001b[0m )\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mprocess_gold_table\u001b[0;34m(source_table, target_table, surrogate_key, delta_load_column, timestamp_key, dim_table_refs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Merge into target table \u001b[39;00m\n\u001b[1;32m     30\u001b[0m merge_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m    MERGE INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AS t\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m    USING tempView AS s\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m        THEN INSERT *\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge"
     ]
    }
   ],
   "source": [
    "process_gold_table (\n",
    "    source_table=\"silver_product\",\n",
    "    target_table=\"gold_dim_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_customer\",\n",
    "    target_table=\"gold_dim_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_sales\",\n",
    "    target_table=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    delta_load_column=\"transaction_time\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_inventory\",\n",
    "    target_table=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    delta_load_column=\"event_time\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
