{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"STREAMING_DWH\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database schemas - Bronze, Silver, Gold\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold;\")\n",
    "spark.sql(\"USE SCHEMA bronze;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def raw_ingestion(schema_name, table_name):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .load(f\"data/{table_name}\")\n",
    "        .withColumn(\"meta_timestamp\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{schema_name}.{table_name}\")\n",
    "        .toTable(f\"{schema_name}.{table_name}\")\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = raw_ingestion(schema_name=\"bronze\", table_name=\"inventory\")\n",
    "query2 = raw_ingestion(schema_name=\"bronze\", table_name=\"product\")\n",
    "query3 = raw_ingestion(schema_name=\"bronze\", table_name=\"purchase\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM bronze.product\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT current_schema()\").show()\n",
    "spark.sql(\"DESCRIBE SCHEMA EXTENDED bronze;\").show()\n",
    "spark.sql(\"SHOW TABLES IN bronze;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze.product\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLOWLY CHANGING DIMENSIONS (SCD) - TYPE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "|category              |cogs|contains_caffeine|contains_fruit|contains_nuts|contains_veggies|event_time                |item            |price|product_id|size  |meta_timestamp         |meta_filename                                        |meta_hashdiff                   |meta_is_current|meta_valid_from|meta_valid_to|\n",
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "|Supercharged Smoothies|2.7 |false            |true          |false        |false           |2024-02-17 19:27:38.255120|Triple Berry Oat|5.99 |SC01      |24 oz.|2024-02-17 20:19:54.95 |file:///home/peter/data/product/1708198058255172.json|5ff3160d534f4a6013382a5f5412be18|NULL           |NULL           |NULL         |\n",
      "|Superfoods Smoothies  |2.1 |false            |true          |false        |true            |2024-02-17 19:27:38.245756|Get Up and Goji |5.99 |SF06      |24 oz.|2024-02-17 20:19:43.361|file:///home/peter/data/product/1708198058245822.json|71177039bc5ec3b9bce344ba3cb8b672|NULL           |NULL           |NULL         |\n",
      "|Superfoods Smoothies  |2.1 |false            |true          |false        |true            |2024-02-17 19:27:38.219851|Island Green    |5.99 |SF01      |24 oz.|2024-02-17 20:19:16.869|file:///home/peter/data/product/1708198058219896.json|46a877223e3c1f223e7ab2a9dca76a3d|NULL           |NULL           |NULL         |\n",
      "|Supercharged Smoothies|2.7 |false            |false         |true         |false           |2024-02-17 19:32:15.991788|Health Nut      |5.99 |SC03      |24 oz.|2024-02-17 20:23:40.153|file:///home/peter/data/product/1708198335991922.json|ae75c0325cd39eb3fb16f95622d441cf|NULL           |NULL           |NULL         |\n",
      "|Classic Smoothies     |1.5 |false            |true          |false        |false           |2024-02-17 19:27:38.214580|Jetty Punch     |4.99 |CS11      |24 oz.|2024-02-17 20:19:11.014|file:///home/peter/data/product/1708198058214631.json|52db46033a77a8c5483162e1bf0c015d|NULL           |NULL           |NULL         |\n",
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import md5, concat_ws, lit\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "\n",
    "# Get list of all columns that doesn't contain \"meta_\"\n",
    "df = spark.sql(\"select * from bronze.product\")\n",
    "df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "df = df.withColumn(\"meta_is_current\", lit(None).cast(BooleanType()))\n",
    "df = df.withColumn(\"meta_valid_from\", lit(None).cast(TimestampType()))\n",
    "df = df.withColumn(\"meta_valid_to\", lit(None).cast(TimestampType()))\n",
    "\n",
    "df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.product_new LIKE tempView\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+--------------+-------------+----------------+----------+----+-----+----------+----+--------------+-------------+-------------+---------------+---------------+-------------+\n",
      "|category|cogs|contains_caffeine|contains_fruit|contains_nuts|contains_veggies|event_time|item|price|product_id|size|meta_timestamp|meta_filename|meta_hashdiff|meta_is_current|meta_valid_from|meta_valid_to|\n",
      "+--------+----+-----------------+--------------+-------------+----------------+----------+----+-----+----------+----+--------------+-------------+-------------+---------------+---------------+-------------+\n",
      "+--------+----+-----------------+--------------+-------------+----------------+----------+----+-----+----------+----+--------------+-------------+-------------+---------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW COLUMNS FROM bronze.product_new\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_table = \"bronze.product\"\n",
    "target_table = \"bronze.product_scd2\"\n",
    "merge_key = \"product_id\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_query = \"\"\"\n",
    "MERGE INTO {target_table}\n",
    "USING (\n",
    "  SELECT {source_table}.*\n",
    "  FROM {source_table} JOIN {target_table}\n",
    "  ON {source_table}.{merge_key} = {target_table}.{merge_key} \n",
    "  WHERE {target_table}.meta_is_current = true {source_table}.meta_hashdiff <> {target_table}.meta_hashdiff \n",
    ") staged_updates\n",
    "ON {target_table}.{merge_key} = staged_updates.{merge_key}\n",
    "WHEN MATCHED AND {target_table}.meta_is_current = true AND {target_table}.meta_hashdiff <> staged_updates.meta_hashdiff THEN  \n",
    "  UPDATE SET meta_is_current = false, endDate = staged_updates.effectiveDate\n",
    "WHEN NOT MATCHED THEN \n",
    "  INSERT(customerid, address, current, effectivedate, enddate)\n",
    "  VALUES(staged_updates.key, staged_updates.address, true, staged_updates.effectiveDate, null)\n",
    "\"\"\".format(\n",
    "    source_table=source_table,\n",
    "    target_table=target_table,\n",
    "    merge_key=merge_key\n",
    "  )\n",
    "\n",
    "print(merge_query)\n",
    "\n",
    "# spark.sql(merge_query)\n",
    "# display(table(\"customers\").orderBy(\"customerId\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
