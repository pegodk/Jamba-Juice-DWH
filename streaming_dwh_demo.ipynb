{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"STREAMING_DWH\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database schemas - Bronze, Silver, Gold\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold;\")\n",
    "spark.sql(\"USE SCHEMA bronze;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def raw_ingestion(schema_name, table_name):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .load(f\"data/{table_name}\")\n",
    "        .withColumn(\"meta_timestamp\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{schema_name}.{table_name}\")\n",
    "        .toTable(f\"{schema_name}.{table_name}\")\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = raw_ingestion(schema_name=\"bronze\", table_name=\"inventory\")\n",
    "query2 = raw_ingestion(schema_name=\"bronze\", table_name=\"product\")\n",
    "query3 = raw_ingestion(schema_name=\"bronze\", table_name=\"purchase\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM bronze.product\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT current_schema()\").show()\n",
    "spark.sql(\"DESCRIBE SCHEMA EXTENDED bronze;\").show()\n",
    "spark.sql(\"SHOW TABLES IN bronze;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze.product\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLOWLY CHANGING DIMENSIONS (SCD) - TYPE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "|category              |cogs|contains_caffeine|contains_fruit|contains_nuts|contains_veggies|event_time                |item            |price|product_id|size  |meta_timestamp         |meta_filename                                        |meta_hashdiff                   |meta_is_current|meta_valid_from|meta_valid_to|\n",
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "|Supercharged Smoothies|2.7 |false            |true          |false        |false           |2024-02-17 19:27:38.255120|Triple Berry Oat|5.99 |SC01      |24 oz.|2024-02-17 20:19:54.95 |file:///home/peter/data/product/1708198058255172.json|5ff3160d534f4a6013382a5f5412be18|NULL           |NULL           |NULL         |\n",
      "|Superfoods Smoothies  |2.1 |false            |true          |false        |true            |2024-02-17 19:27:38.245756|Get Up and Goji |5.99 |SF06      |24 oz.|2024-02-17 20:19:43.361|file:///home/peter/data/product/1708198058245822.json|71177039bc5ec3b9bce344ba3cb8b672|NULL           |NULL           |NULL         |\n",
      "|Superfoods Smoothies  |2.1 |false            |true          |false        |true            |2024-02-17 19:27:38.219851|Island Green    |5.99 |SF01      |24 oz.|2024-02-17 20:19:16.869|file:///home/peter/data/product/1708198058219896.json|46a877223e3c1f223e7ab2a9dca76a3d|NULL           |NULL           |NULL         |\n",
      "|Supercharged Smoothies|2.7 |false            |false         |true         |false           |2024-02-17 19:32:15.991788|Health Nut      |5.99 |SC03      |24 oz.|2024-02-17 20:23:40.153|file:///home/peter/data/product/1708198335991922.json|ae75c0325cd39eb3fb16f95622d441cf|NULL           |NULL           |NULL         |\n",
      "|Classic Smoothies     |1.5 |false            |true          |false        |false           |2024-02-17 19:27:38.214580|Jetty Punch     |4.99 |CS11      |24 oz.|2024-02-17 20:19:11.014|file:///home/peter/data/product/1708198058214631.json|52db46033a77a8c5483162e1bf0c015d|NULL           |NULL           |NULL         |\n",
      "+----------------------+----+-----------------+--------------+-------------+----------------+--------------------------+----------------+-----+----------+------+-----------------------+-----------------------------------------------------+--------------------------------+---------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import md5, concat_ws, lit\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "\n",
    "# Get list of all columns that doesn't contain \"meta_\"\n",
    "df = spark.sql(\"select * from bronze.product\")\n",
    "df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "df = df.withColumn(\"meta_is_current\", lit(None).cast(BooleanType()))\n",
    "df = df.withColumn(\"meta_valid_from\", lit(None).cast(TimestampType()))\n",
    "df = df.withColumn(\"meta_valid_to\", lit(None).cast(TimestampType()))\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Create an empty Delta table with the same schema\n",
    "df.createOrReplaceTempView(\"tempView\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.product_scd2 LIKE tempView USING DELTA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE bronze.product_scd2\").show()\n",
    "# spark.sql(\"SHOW COLUMNS FROM bronze.product_scd2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table = \"tempView\"\n",
    "target_table = \"bronze.product_scd2\"\n",
    "merge_key = \"product_id\"\n",
    "timestamp = \"event_time\"\n",
    "\n",
    "spark.sql(\"select * from tempView\").show()\n",
    "\n",
    "# merge_query = \"\"\"\n",
    "#     SELECT {source_table}.*\n",
    "#     FROM {source_table} JOIN {target_table}\n",
    "#     ON {source_table}.{merge_key} = {target_table}.{merge_key} \n",
    "#     WHERE {source_table}.meta_hashdiff <> {target_table}.meta_hashdiff \n",
    "# \"\"\".format(\n",
    "#     source_table=source_table,\n",
    "#     target_table=target_table,\n",
    "#     merge_key=merge_key,\n",
    "#     timestamp=timestamp\n",
    "#   )\n",
    "\n",
    "# spark.sql(merge_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  MERGE INTO bronze.product_scd2 AS target\n",
      "  USING tempView AS source ON target.product_id = source.product_id\n",
      "  WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff \n",
      "  THEN UPDATE SET meta_is_current = false, meta_valid_to = source.event_time\n",
      "  WHEN NOT MATCHED THEN INSERT\n",
      "  (\n",
      "    product_id, \n",
      "    category, \n",
      "    cogs, \n",
      "    contains_caffeine, \n",
      "    contains_fruit, \n",
      "    contains_nuts, \n",
      "    meta_is_current, \n",
      "    meta_valid_from, \n",
      "    meta_valid_to\n",
      "  )\n",
      "  VALUES\n",
      "  (\n",
      "    source.product_id, \n",
      "    source.category, \n",
      "    source.cogs, \n",
      "    source.contains_caffeine, \n",
      "    source.contains_fruit, \n",
      "    source.contains_nuts,\n",
      "    TRUE,\n",
      "    source.event_time,\n",
      "    NULL\n",
      "  )\n",
      "\n"
     ]
    },
    {
     "ename": "UnsupportedOperationException",
     "evalue": "MERGE INTO TABLE is not supported temporarily.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(merge_query)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# spark.sql(merge_query)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# display(table(\"customers\").orderBy(\"customerId\"))\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: MERGE INTO TABLE is not supported temporarily."
     ]
    }
   ],
   "source": [
    "\n",
    "source_table = \"tempView\"\n",
    "target_table = \"bronze.product_scd2\"\n",
    "merge_key = \"product_id\"\n",
    "timestamp = \"event_time\"\n",
    "\n",
    "\n",
    "# |MERGE INTO numbers_merge_target AS target\n",
    "# | USING numbers_merge_source AS source ON target.number = source.number\n",
    "# |WHEN MATCHED THEN UPDATE SET target.number = source.number\n",
    "\n",
    "merge_query = \"\"\"\n",
    "  MERGE INTO {target_table} AS target\n",
    "  USING {source_table} AS source ON target.{merge_key} = source.{merge_key}\n",
    "  WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff \n",
    "  THEN UPDATE SET meta_is_current = false, meta_valid_to = source.{timestamp}\n",
    "  WHEN NOT MATCHED THEN INSERT\n",
    "  (\n",
    "    product_id, \n",
    "    category, \n",
    "    cogs, \n",
    "    contains_caffeine, \n",
    "    contains_fruit, \n",
    "    contains_nuts, \n",
    "    meta_is_current, \n",
    "    meta_valid_from, \n",
    "    meta_valid_to\n",
    "  )\n",
    "  VALUES\n",
    "  (\n",
    "    source.product_id, \n",
    "    source.category, \n",
    "    source.cogs, \n",
    "    source.contains_caffeine, \n",
    "    source.contains_fruit, \n",
    "    source.contains_nuts,\n",
    "    TRUE,\n",
    "    source.{timestamp},\n",
    "    NULL\n",
    "  )\n",
    "\"\"\".format(\n",
    "    source_table=source_table,\n",
    "    target_table=target_table,\n",
    "    merge_key=merge_key,\n",
    "    timestamp=timestamp\n",
    "  )\n",
    "\n",
    "print(merge_query)\n",
    "\n",
    "# spark.sql(merge_query)\n",
    "# display(table(\"customers\").orderBy(\"customerId\"))\n",
    "\n",
    "spark.sql(merge_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_table = \"tempView\"\n",
    "target_table = \"bronze.product_scd2\"\n",
    "merge_key = \"product_id\"\n",
    "timestamp = \"event_time\"\n",
    "\n",
    "\n",
    "# |MERGE INTO numbers_merge_target AS target\n",
    "# | USING numbers_merge_source AS source ON target.number = source.number\n",
    "# |WHEN MATCHED THEN UPDATE SET target.number = source.number\n",
    "\n",
    "merge_query = \"\"\"\n",
    "  MERGE INTO {target_table}\n",
    "  USING (\n",
    "    SELECT {source_table}.*\n",
    "    FROM {source_table} JOIN {target_table}\n",
    "    ON {source_table}.{merge_key} = {target_table}.{merge_key} \n",
    "    WHERE {target_table}.meta_is_current = true\n",
    "    AND {source_table}.meta_hashdiff <> {target_table}.meta_hashdiff \n",
    "  ) staged_updates\n",
    "  ON {target_table}.{merge_key} = staged_updates.{merge_key}\n",
    "  WHEN MATCHED AND {target_table}.meta_is_current = true \n",
    "  AND {target_table}.meta_hashdiff <> staged_updates.meta_hashdiff THEN\n",
    "    UPDATE SET meta_is_current = false, meta_valid_to = staged_updates.{timestamp}\n",
    "  WHEN NOT MATCHED THEN INSERT\n",
    "  (\n",
    "    product_id, \n",
    "    category, \n",
    "    cogs, \n",
    "    contains_caffeine, \n",
    "    contains_fruit, \n",
    "    contains_nuts, \n",
    "    meta_is_current, \n",
    "    meta_valid_from, \n",
    "    meta_valid_to\n",
    "  )\n",
    "  VALUES\n",
    "  (\n",
    "    staged_updates.product_id, \n",
    "    staged_updates.category, \n",
    "    staged_updates.cogs, \n",
    "    staged_updates.contains_caffeine, \n",
    "    staged_updates.contains_fruit, \n",
    "    staged_updates.contains_nuts, \n",
    "    TRUE,\n",
    "    staged_updates.{timestamp}, \n",
    "    NULL\n",
    "  )\n",
    "\"\"\".format(\n",
    "    source_table=source_table,\n",
    "    target_table=target_table,\n",
    "    merge_key=merge_key,\n",
    "    timestamp=timestamp\n",
    "  )\n",
    "\n",
    "print(merge_query)\n",
    "\n",
    "# spark.sql(merge_query)\n",
    "# display(table(\"customers\").orderBy(\"customerId\"))\n",
    "\n",
    "spark.sql(merge_query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
