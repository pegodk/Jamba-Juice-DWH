{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO STREAMING DATAWAREHOUSE\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- cogs: double (nullable = true)\n",
      " |-- contains_caffeine: boolean (nullable = true)\n",
      " |-- contains_fruit: boolean (nullable = true)\n",
      " |-- contains_nuts: boolean (nullable = true)\n",
      " |-- contains_veggies: boolean (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bronze Tables\n",
    "This function will create empty tables for future use.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- meta_created\n",
    "- meta_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def create_bronze_streaming_table(source, target):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target}\")\n",
    "        .toTable(target)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "query3 = create_bronze_streaming_table(source=\"data/sales\", target=\"bronze_sales\")\n",
    "query4 = create_bronze_streaming_table(source=\"data/customer\", target=\"bronze_customer\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     339|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM bronze_sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Silver Tables\n",
    "This function will create empty tables for future use.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following columns will be created:\n",
    "\n",
    "SCD type 1:\n",
    "- meta_hashdiff\n",
    "- meta_last_updated\n",
    "- meta_sequence\n",
    "\n",
    "SCD type 2:\n",
    "- meta_hashdiff\n",
    "- meta_is_current\n",
    "- meta_valid_from\n",
    "- meta_valid_to\n",
    "- meta_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE OR REPLACE TABLE silver_sales_scd1 (transaction_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE OR REPLACE TABLE silver_inventory_scd1 (inventory_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE OR REPLACE TABLE silver_product_scd2 (product_sid string, category string, cogs double, contains_caffeine boolean, contains_fruit boolean, contains_nuts boolean, contains_veggies boolean, event_time string, item string, price double, product_id string, size string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n",
      "CREATE OR REPLACE TABLE silver_customer_scd2 (customer_sid string, address string, credit_card_expire string, credit_card_number string, customer_id bigint, email string, event_time string, full_name string, phone_number string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n"
     ]
    }
   ],
   "source": [
    "def create_silver_table(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    # query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    query = f\"CREATE OR REPLACE TABLE {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "create_silver_table(table_name=\"silver_sales_scd1\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "create_silver_table(table_name=\"silver_inventory_scd1\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "create_silver_table(table_name=\"silver_product_scd2\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "create_silver_table(table_name=\"silver_customer_scd2\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      product_sid|   string|   NULL|\n",
      "|         category|   string|   NULL|\n",
      "|             cogs|   double|   NULL|\n",
      "|contains_caffeine|  boolean|   NULL|\n",
      "|   contains_fruit|  boolean|   NULL|\n",
      "|    contains_nuts|  boolean|   NULL|\n",
      "| contains_veggies|  boolean|   NULL|\n",
      "|       event_time|   string|   NULL|\n",
      "|             item|   string|   NULL|\n",
      "|            price|   double|   NULL|\n",
      "|       product_id|   string|   NULL|\n",
      "|             size|   string|   NULL|\n",
      "|     meta_created|timestamp|   NULL|\n",
      "|    meta_filename|   string|   NULL|\n",
      "|    meta_hashdiff|   string|   NULL|\n",
      "|  meta_is_current|  boolean|   NULL|\n",
      "|  meta_valid_from|timestamp|   NULL|\n",
      "|    meta_valid_to|timestamp|   NULL|\n",
      "|    meta_sequence|      int|   NULL|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table silver_product_scd2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Tables: Slowly Changing Dimensions (SCD) Type 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def create_silver_scd1_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    timestamp_key : str,\n",
    "    merge_key: str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "    \n",
    "    # Perform delta load\n",
    "    df = spark.sql(f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {source} \n",
    "            WHERE {delta_load_column} > (\n",
    "                SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target}\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # Calculate surrogate key as hash of natural key columns\n",
    "    df = df.withColumn(surrogate_key, md5(merge_key))\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta_last_updated\n",
    "    df = df.withColumn(\"meta_last_updated\", current_timestamp())\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "    \n",
    "    # Create view with source data\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 1 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT *\n",
    "                FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{surrogate_key} = s.{surrogate_key}\n",
    "            WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code to generate silver_sales_scd1 and silver_inventory_scd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_sales_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|             1114|               0|               0|             1114|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_inventory_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              219|               0|               0|              219|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_sales\",\n",
    "    target=\"silver_sales_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    merge_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_inventory\",\n",
    "    target=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    merge_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_scd2_table(\n",
    "    source: str, \n",
    "    target: str, \n",
    "    merge_key: str, \n",
    "    timestamp_key: str, \n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "    \n",
    "    # Perform delta load\n",
    "    df = spark.sql(f\"\"\"\n",
    "            SELECT * \n",
    "            FROM {source} \n",
    "            WHERE {delta_load_column} > (\n",
    "                SELECT COALESCE(max({delta_load_column}), '1970-01-01') FROM {target}\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    # Calculate surrogate key as hash of natural key columns\n",
    "    df = df.withColumn(surrogate_key, md5(merge_key))\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 2 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{merge_key} = s.{merge_key}\n",
    "            WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target}\n",
    "            SELECT * \n",
    "            FROM \n",
    "            (\n",
    "                SELECT s.*\n",
    "                FROM {tmp_view_name} s\n",
    "                JOIN {target} t ON t.{merge_key} = s.{merge_key}\n",
    "                WHERE s.meta_sequence = {seq_num}\n",
    "                AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            )\n",
    "        \"\"\"\n",
    "        spark.sql(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"md5(customer_id)\" due to data type mismatch: Parameter 1 requires the \"BINARY\" type, however \"customer_id\" has the type \"BIGINT\".;\n'Project [address#981570, credit_card_expire#981571, credit_card_number#981572, customer_id#981573L, email#981574, event_time#981575, full_name#981576, phone_number#981577, meta_created#981578, meta_filename#981579, md5(customer_id#981573L) AS customer_sid#981610]\n+- Project [address#981570, credit_card_expire#981571, credit_card_number#981572, customer_id#981573L, email#981574, event_time#981575, full_name#981576, phone_number#981577, meta_created#981578, meta_filename#981579]\n   +- Filter (event_time#981575 > scalar-subquery#981502 [])\n      :  +- Aggregate [coalesce(max(event_time#981586), 1970-01-01) AS coalesce(max(event_time), 1970-01-01)#981597]\n      :     +- SubqueryAlias spark_catalog.default.silver_customer_scd2\n      :        +- Relation spark_catalog.default.silver_customer_scd2[customer_sid#981580,address#981581,credit_card_expire#981582,credit_card_number#981583,customer_id#981584L,email#981585,event_time#981586,full_name#981587,phone_number#981588,meta_created#981589,meta_filename#981590,meta_hashdiff#981591,meta_is_current#981592,meta_valid_from#981593,meta_valid_to#981594,meta_sequence#981595] parquet\n      +- SubqueryAlias spark_catalog.default.bronze_customer\n         +- Relation spark_catalog.default.bronze_customer[address#981570,credit_card_expire#981571,credit_card_number#981572,customer_id#981573L,email#981574,event_time#981575,full_name#981576,phone_number#981577,meta_created#981578,meta_filename#981579] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create SCD2 tables\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# create_silver_scd2_table(\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     source = \"bronze_product\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     delta_load_column=\"event_time\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcreate_silver_scd2_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbronze_customer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilver_customer_scd2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_sid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta_load_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 20\u001b[0m, in \u001b[0;36mcreate_silver_scd2_table\u001b[0;34m(source, target, merge_key, timestamp_key, surrogate_key, delta_load_column)\u001b[0m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m        SELECT * \u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m        FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m        )\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate surrogate key as hash of natural key columns\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurrogate_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta_hashdiff\u001b[39m\u001b[38;5;124m\"\u001b[39m, md5(concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m||\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m c])))\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"md5(customer_id)\" due to data type mismatch: Parameter 1 requires the \"BINARY\" type, however \"customer_id\" has the type \"BIGINT\".;\n'Project [address#981570, credit_card_expire#981571, credit_card_number#981572, customer_id#981573L, email#981574, event_time#981575, full_name#981576, phone_number#981577, meta_created#981578, meta_filename#981579, md5(customer_id#981573L) AS customer_sid#981610]\n+- Project [address#981570, credit_card_expire#981571, credit_card_number#981572, customer_id#981573L, email#981574, event_time#981575, full_name#981576, phone_number#981577, meta_created#981578, meta_filename#981579]\n   +- Filter (event_time#981575 > scalar-subquery#981502 [])\n      :  +- Aggregate [coalesce(max(event_time#981586), 1970-01-01) AS coalesce(max(event_time), 1970-01-01)#981597]\n      :     +- SubqueryAlias spark_catalog.default.silver_customer_scd2\n      :        +- Relation spark_catalog.default.silver_customer_scd2[customer_sid#981580,address#981581,credit_card_expire#981582,credit_card_number#981583,customer_id#981584L,email#981585,event_time#981586,full_name#981587,phone_number#981588,meta_created#981589,meta_filename#981590,meta_hashdiff#981591,meta_is_current#981592,meta_valid_from#981593,meta_valid_to#981594,meta_sequence#981595] parquet\n      +- SubqueryAlias spark_catalog.default.bronze_customer\n         +- Relation spark_catalog.default.bronze_customer[address#981570,credit_card_expire#981571,credit_card_number#981572,customer_id#981573L,email#981574,event_time#981575,full_name#981576,phone_number#981577,meta_created#981578,meta_filename#981579] parquet\n"
     ]
    }
   ],
   "source": [
    "# Create SCD2 tables\n",
    "# create_silver_scd2_table(\n",
    "#     source = \"bronze_product\",\n",
    "#     target = \"silver_product_scd2\",\n",
    "#     merge_key = \"product_id\",\n",
    "#     timestamp_key = \"event_time\",\n",
    "#     surrogate_key = \"product_sid\",\n",
    "#     delta_load_column=\"event_time\"\n",
    "# )\n",
    "\n",
    "create_silver_scd2_table(\n",
    "    source = \"bronze_customer\",\n",
    "    target = \"silver_customer_scd2\",\n",
    "    merge_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------------+-----------+--------------------+--------------------+-------------+---------------+--------------------+--------------------+\n",
      "|             address|credit_card_expire| credit_card_number|customer_id|               email|          event_time|    full_name|   phone_number|        meta_created|       meta_filename|\n",
      "+--------------------+------------------+-------------------+-----------+--------------------+--------------------+-------------+---------------+--------------------+--------------------+\n",
      "|980 Miller Ford\\n...|             05/32|4443642719737473246|          1|larsenrichard@exa...|2024-04-15 19:23:...|Derek Simmons|+1-455-855-3505|2024-04-15 19:26:...|file:///home/pete...|\n",
      "|980 Miller Ford\\n...|             05/32|4443642719737473246|          1|larsenrichard@exa...|2024-04-15 19:33:...|Derek Simmons|+1-455-855-3505|2024-04-15 19:33:...|file:///home/pete...|\n",
      "|980 Miller Ford\\n...|             02/30|   2355423216440553|          1|larsenrichard@exa...|2024-04-15 19:20:...|Derek Simmons|+1-455-855-3505|2024-04-15 19:26:...|file:///home/pete...|\n",
      "|980 Miller Ford\\n...|             05/32|4443642719737473246|          1|larsenrichard@exa...|2024-04-15 19:21:...|Derek Simmons|+1-455-855-3505|2024-04-15 19:26:...|file:///home/pete...|\n",
      "|980 Miller Ford\\n...|             09/27|   4175720948637347|          1|larsenrichard@exa...|2024-04-15 19:20:...|Derek Simmons|+1-455-855-3505|2024-04-15 19:26:...|file:///home/pete...|\n",
      "+--------------------+------------------+-------------------+-----------+--------------------+--------------------+-------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from bronze_customer order by customer_id\").show(5)\n",
    "\n",
    "# spark.sql(\"select * from silver_sales_scd1\").show(5)\n",
    "# spark.sql(\"select * from silver_inventory_scd1\").show(5)\n",
    "# spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(5)\n",
    "# spark.sql(\"select * from silver_customer_scd2 order by customer_sid\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE GOLD TABLES - FACTS AND DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS gold_fact_sales (transaction_sid string, product_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n",
      "CREATE TABLE IF NOT EXISTS gold_fact_inventory (inventory_sid string, product_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_gold_table(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "    return\n",
    "\n",
    "\n",
    "create_gold_table (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")\n",
    "\n",
    "create_gold_table (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_utils import generate_dim_table_references\n",
    "\n",
    "def create_gold_fact_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source,\n",
    "                                                 target=target,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|             1114|               0|               0|             1114|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              219|               0|               0|              219|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_gold_fact_table (\n",
    "    source=\"silver_sales_scd1\",\n",
    "    target=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "create_gold_fact_table (\n",
    "    source=\"silver_inventory_scd1\",\n",
    "    target=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|       inventory_sid|         product_sid|          event_time|existing_level|new_level|product_id|stock_quantity|        meta_created|       meta_filename|       meta_hashdiff|   meta_last_updated|meta_sequence|\n",
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|de6c6db2725bd3f91...|749800f100d37411d...|2024-04-15 19:20:...|            32|       42|      SC01|            10|2024-04-15 19:26:...|file:///home/pete...|5469e1ee7d1f8c6c9...|2024-04-15 19:54:...|            1|\n",
      "|3caf11f62becf2cc5...|e70b3bf706cd880d8...|2024-04-15 19:20:...|            34|       44|      SC03|            10|2024-04-15 19:26:...|file:///home/pete...|0fe5c45bf9ce720d6...|2024-04-15 19:54:...|            1|\n",
      "|b82041d0eddc195dd...|dd479b3f57cfdef76...|2024-04-15 19:20:...|            32|       42|      SC05|            10|2024-04-15 19:26:...|file:///home/pete...|36b0c5f614f55db9b...|2024-04-15 19:54:...|            1|\n",
      "|4c7e01bd83bc633f5...|255593ad608445b41...|2024-04-15 19:20:...|            33|       43|      SC04|            10|2024-04-15 19:26:...|file:///home/pete...|baba78ffac09a36fe...|2024-04-15 19:54:...|            1|\n",
      "|dd83d81b019fa2ea5...|dd479b3f57cfdef76...|2024-04-15 19:20:...|            41|       51|      SC05|            10|2024-04-15 19:26:...|file:///home/pete...|7f8a42f6e9b85ac89...|2024-04-15 19:54:...|            1|\n",
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"select * from gold_fact_sales\").show(5)\n",
    "spark.sql(\"select * from gold_fact_inventory\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_sales_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              636|               0|               0|              636|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_inventory_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              129|               0|               0|              129|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              636|               0|               0|              636|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              129|               0|               0|              129|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_sales_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|               60|               0|               0|               60|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# SILVER SCD TYPE 1 & 2 TABLES\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     create_silver_scd1_table (\n\u001b[1;32m      5\u001b[0m         source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_sales_scd1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         delta_load_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mcreate_silver_scd1_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbronze_inventory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilver_inventory_scd1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestamp_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmerge_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43msurrogate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minventory_sid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta_load_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     create_silver_scd2_table(\n\u001b[1;32m     21\u001b[0m         source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_product\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_product_scd2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         delta_load_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# GOLD FACT AND DIMENSION TABLES\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m, in \u001b[0;36mcreate_silver_scd1_table\u001b[0;34m(source, target, timestamp_key, merge_key, surrogate_key, delta_load_column)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_silver_scd1_table\u001b[39m(\n\u001b[1;32m      6\u001b[0m     source : \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m      7\u001b[0m     target : \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Perform delta load\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m            SELECT * \u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m            FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m            WHERE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdelta_load_column\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m > (\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m                SELECT COALESCE(MAX(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdelta_load_column\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m), \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1970-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m            )\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Calculate surrogate key as hash of natural key columns\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(surrogate_key, md5(merge_key))\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    # SILVER SCD TYPE 1 & 2 TABLES\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_sales\",\n",
    "        target=\"silver_sales_scd1\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        merge_key=\"transaction_id\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        delta_load_column=\"transaction_time\"\n",
    "    )\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_inventory\",\n",
    "        target=\"silver_inventory_scd1\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        merge_key=\"event_time\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "    create_silver_scd2_table(\n",
    "        source = \"bronze_product\",\n",
    "        target = \"silver_product_scd2\",\n",
    "        merge_key = \"product_id\",\n",
    "        timestamp_key = \"event_time\",\n",
    "        surrogate_key = \"product_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "\n",
    "    # GOLD FACT AND DIMENSION TABLES\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_sales_scd1\",\n",
    "        target=\"gold_fact_sales\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "        delta_load_column=\"transaction_time\"\n",
    "    )\n",
    "\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_inventory_scd1\",\n",
    "        target=\"gold_fact_inventory\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "        delta_load_column=\"event_time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
