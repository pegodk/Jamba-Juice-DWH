{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Warehousing Demo\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n",
    "\n",
    "### Bronze layer (raw data)\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "### Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.\n",
    "\n",
    "### Gold layer (curated business-level tables)\n",
    "Data in the Gold layer of the lakehouse is typically organized in consumption-ready \"project-specific\" databases. The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here. Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer. We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit in this Gold Layer of the lakehouse.\n",
    "\n",
    "So you can see that the data is curated as it moves through the different layers of a lakehouse. In some cases, we also see that lot of Data Marts and EDWs from the traditional RDBMS technology stack are ingested into the lakehouse, so that for the first time Enterprises can do \"pan-EDW\" advanced analytics and ML - which was just not possible or too cost prohibitive to do on a traditional stack. (e.g. IoT/Manufacturing data is tied with Sales and Marketing data for defect analysis or health care genomics, EMR/HL7 clinical data markets are tied with financial claims data to create a Healthcare Data Lake for timely and improved patient care analytics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Data Ingestion\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- **meta_created**: Timestamp from when the row was ingested\n",
    "- **meta_filename**: Filename from which the row was ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "def bronze_streaming_table(source_folder, target_table):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source_folder)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .trigger(processingTime='10 seconds')\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target_table}\")\n",
    "        .queryName(target_table)\n",
    "        .toTable(target_table)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "bronze_streaming_table(source_folder=\"data/inventory\", target_table=\"bronze_inventory\")\n",
    "bronze_streaming_table(source_folder=\"data/product\", target_table=\"bronze_product\")\n",
    "bronze_streaming_table(source_folder=\"data/sales\", target_table=\"bronze_sales\")\n",
    "bronze_streaming_table(source_folder=\"data/customer\", target_table=\"bronze_customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Slowly Changing Dimensions\n",
    "Type 1 Slowly Changing Dimension: This method overwrites the existing value with the new value and does not retain history. Type 2 Slowly Changing Dimension: This method adds a new row for the new value and maintains the existing row for historical and reporting purposes.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following meta columns will be created:\n",
    "\n",
    "### SCD Type 1:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns. [Read more here.](https://www.tpximpact.com/knowledge-hub/blogs/tech/hash-keys-data-warehousing-2/)\n",
    "- **meta_last_updated**: Timestamp of when the row was last updated/overwritten.\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key.\n",
    "\n",
    "### SCD Type 2:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns.\n",
    "- **meta_is_current**: Boolean of whether this row is the current (most recent).\n",
    "- **meta_valid_from**: Timestamp of when the row was first ingested.\n",
    "- **meta_valid_to**: Timestamp of then the row was outdated by a newer version (Null if row is current).\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    # Run and print SQL query\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "\n",
    "silver_table_schema(table_name=\"silver_sales\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_inventory\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_product\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "silver_table_schema(table_name=\"silver_customer\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 1 & 2\n",
    "\n",
    "In the Type 1 SCD, you simply overwrite data in dimensions. [Read more here.](https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def upsert_to_scd_table(\n",
    "        target_table: str,\n",
    "        timestamp_key: str,\n",
    "        join_key: str,\n",
    "        scd_type: int\n",
    "    ):\n",
    "\n",
    "    def _inner_func(df, batch_id):\n",
    "        \n",
    "        # Set default values for meta columns depending on SCD type\n",
    "        if scd_type == 1:\n",
    "            df  = df.withColumn(\"meta_last_updated\", current_timestamp())\n",
    "        elif scd_type == 2:\n",
    "            df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "            df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "            df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "        else:\n",
    "            raise ValueError(\"Currently only supports SCD type 1 and 2\")\n",
    "        \n",
    "        # Calculate hashdiff\n",
    "        df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "        # Calculate sequence number\n",
    "        df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "        # Reorder columns to match target table (only neccessary for SCD type 2)\n",
    "        if scd_type == 2:\n",
    "            df_target = spark.read.table(target_table).limit(1)\n",
    "            df = df.select(df_target.columns)\n",
    "\n",
    "        # Create view with source data\n",
    "        df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "        # Get list of sequences\n",
    "        lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "        # Loop over the sequence and do merge into statements for either SCD type 1 or 2\n",
    "        for seq_num in lst_sequence:\n",
    "            if scd_type == 1:\n",
    "                query = f\"\"\"\n",
    "                    MERGE INTO {target_table} AS t\n",
    "                    USING (\n",
    "                        SELECT *\n",
    "                        FROM tempView\n",
    "                        WHERE meta_sequence = {seq_num}\n",
    "                    ) AS s ON t.{join_key} = s.{join_key}\n",
    "                    WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                        THEN UPDATE SET *\n",
    "                    WHEN NOT MATCHED \n",
    "                        THEN INSERT *\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(query)\n",
    "\n",
    "            elif scd_type == 2:\n",
    "                merge_query = f\"\"\"\n",
    "                    MERGE INTO {target_table} AS t\n",
    "                    USING (\n",
    "                        SELECT * \n",
    "                        FROM tempView\n",
    "                        WHERE meta_sequence = {seq_num}\n",
    "                    ) AS s ON t.{join_key} = s.{join_key}\n",
    "                    WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                        THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "                    WHEN NOT MATCHED \n",
    "                        THEN INSERT *\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(merge_query)\n",
    "\n",
    "                insert_query = f\"\"\"\n",
    "                    INSERT INTO {target_table}\n",
    "                    SELECT s.*\n",
    "                    FROM tempView s\n",
    "                    JOIN {target_table} t ON t.{join_key} = s.{join_key}\n",
    "                    WHERE s.meta_sequence = {seq_num}\n",
    "                    AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                \"\"\"\n",
    "                df.sparkSession.sql(insert_query)\n",
    "    \n",
    "    return _inner_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str,\n",
    "    scd_type: int\n",
    "):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd_table(\n",
    "            target_table=target_table, \n",
    "            timestamp_key=timestamp_key, \n",
    "            join_key=join_key, \n",
    "            scd_type=scd_type\n",
    "        ))\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver tables with SCD type 1\n",
    "silver_streaming_table (\n",
    "    source_table=\"bronze_sales\",\n",
    "    target_table=\"silver_sales\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    join_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    scd_type=1\n",
    ")\n",
    "\n",
    "silver_streaming_table (\n",
    "    source_table=\"bronze_inventory\",\n",
    "    target_table=\"silver_inventory\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    join_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    scd_type=1\n",
    ")\n",
    "\n",
    "# Create silver tables with SCD type 2\n",
    "silver_streaming_table(\n",
    "    source_table = \"bronze_product\",\n",
    "    target_table = \"silver_product\",\n",
    "    join_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\",\n",
    "    scd_type=2\n",
    ")\n",
    "\n",
    "silver_streaming_table(\n",
    "    source_table = \"bronze_customer\",\n",
    "    target_table = \"silver_customer\",\n",
    "    join_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\",\n",
    "    scd_type=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Facts and Dimensions\n",
    "## What is Dimensional Modeling\n",
    "The data model used to store data in the denormalized form is called Dimensional Modeling. It is the technique of storing data in a Data Warehouse in such a way that enables fast query performance and easy access to its business users. It involves creating a set of dimensional tables that are designed to support business intelligence and reporting needs.\n",
    "\n",
    "The core concept of dimensional modeling is the creation of a star schema. It is called so as the tables are arranged in the form of a star.\n",
    "\n",
    "![alt text](images/star_schema.png \"Star Schema\")\n",
    "\n",
    "Dimensional modeling includes facts and dimensions. Let’s have a basic idea of what Facts and Dimensions are.\n",
    "\n",
    "## Fact Tables\n",
    "Fact tables are the heart of a data warehouse. They contain quantitative data, often referred to as measures or metrics, and are the focus of most data analysis. These tables store data related to business transactions and events, such as sales figures, revenue, or quantities sold. In essence, fact tables provide the “what” in data analysis.\n",
    "\n",
    "## Dimension Tables\n",
    "Dimension tables, on the other hand, offer context to the data stored in fact tables. They provide descriptive information that helps users understand the “who,” “where,” and “when” aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict,\n",
    "        include_unknown_column : bool\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE OR REPLACE TABLE {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "    if include_unknown_column:\n",
    "        print(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n",
    "        spark.sql(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_product\",\n",
    "    source_table=\"silver_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_customer\",\n",
    "    source_table=\"silver_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    include_unknown_column=False\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    include_unknown_column=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_func_utils import generate_dim_table_references\n",
    "\n",
    "def process_gold_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column : str = None,\n",
    "    timestamp_key : str = \"\",\n",
    "    dim_table_refs : list[dict] = []\n",
    "):\n",
    "    print(\"\\nProcessesing changes into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    if \"fact\" in target_table:\n",
    "        query = generate_dim_table_references(\n",
    "            source=source_table,\n",
    "            target=target_table,\n",
    "            timestamp_key=timestamp_key, \n",
    "            dim_table_refs=dim_table_refs, \n",
    "            delta_load_column=delta_load_column)\n",
    "    else:\n",
    "        query = f\"SELECT * FROM {source_table} s\"\n",
    "\n",
    "    # Add delta load logic if the target_table table already exists\n",
    "    if delta_load_column:\n",
    "        query += f\"\\n WHERE s.{delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target_table})\"\n",
    "\n",
    "    # Run query and create view\n",
    "    spark.sql(query).createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gold_table (\n",
    "    source_table=\"silver_product\",\n",
    "    target_table=\"gold_dim_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_customer\",\n",
    "    target_table=\"gold_dim_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_sales\",\n",
    "    target_table=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    delta_load_column=\"transaction_time\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "process_gold_table (\n",
    "    source_table=\"silver_inventory\",\n",
    "    target_table=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    delta_load_column=\"event_time\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
