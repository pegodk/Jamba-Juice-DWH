{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Warehousing Demo\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n",
    "\n",
    "## Bronze layer (raw data)\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "## Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer.\n",
    "\n",
    "## Gold layer (curated business-level tables)\n",
    "Data in the Gold layer of the lakehouse is typically organized in consumption-ready \"project-specific\" databases. The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here. Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer. We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit in this Gold Layer of the lakehouse.\n",
    "\n",
    "So you can see that the data is curated as it moves through the different layers of a lakehouse. In some cases, we also see that lot of Data Marts and EDWs from the traditional RDBMS technology stack are ingested into the lakehouse, so that for the first time Enterprises can do \"pan-EDW\" advanced analytics and ML - which was just not possible or too cost prohibitive to do on a traditional stack. (e.g. IoT/Manufacturing data is tied with Sales and Marketing data for defect analysis or health care genomics, EMR/HL7 clinical data markets are tied with financial claims data to create a Healthcare Data Lake for timely and improved patient care analytics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Data Ingestion\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- **meta_created**: Timestamp from when the row was ingested\n",
    "- **meta_filename**: Filename from which the row was ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name bronze_inventory as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 25\u001b[0m\n\u001b[1;32m      9\u001b[0m     query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m         spark\u001b[38;5;241m.\u001b[39mreadStream\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mtoTable(target_table)\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query\n\u001b[0;32m---> 25\u001b[0m \u001b[43mcreate_bronze_streaming_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/inventory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbronze_inventory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m create_bronze_streaming_table(source_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/product\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_product\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m create_bronze_streaming_table(source_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/sales\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mcreate_bronze_streaming_table\u001b[0;34m(source_folder, target_table)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_bronze_streaming_table\u001b[39m(source_folder, target_table):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Generates a source path based on table name, reads all files from that and inserts into bronze schema\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m         \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_created\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_filename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark-warehouse/_checkpoints/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_table\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:1622\u001b[0m, in \u001b[0;36mDataStreamWriter.toTable\u001b[0;34m(self, tableName, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m queryName \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m-> 1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name bronze_inventory as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "def bronze_streaming_table(source_folder, target_table):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source_folder)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .trigger(processingTime='10 seconds')\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target_table}\")\n",
    "        .queryName(target_table)\n",
    "        .toTable(target_table)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "bronze_streaming_table(source_folder=\"data/inventory\", target_table=\"bronze_inventory\")\n",
    "bronze_streaming_table(source_folder=\"data/product\", target_table=\"bronze_product\")\n",
    "bronze_streaming_table(source_folder=\"data/sales\", target_table=\"bronze_sales\")\n",
    "bronze_streaming_table(source_folder=\"data/customer\", target_table=\"bronze_customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Slowly Changing Dimensions\n",
    "Type 1 Slowly Changing Dimension: This method overwrites the existing value with the new value and does not retain history. Type 2 Slowly Changing Dimension: This method adds a new row for the new value and maintains the existing row for historical and reporting purposes.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following meta columns will be created:\n",
    "\n",
    "SCD Type 1:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns. [Read more here.](https://www.tpximpact.com/knowledge-hub/blogs/tech/hash-keys-data-warehousing-2/)\n",
    "- **meta_last_updated**: Timestamp of when the row was last updated/overwritten.\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key.\n",
    "\n",
    "SCD Type 2:\n",
    "- **meta_hashdiff**: Hash key of all non-meta columns. [Read more here.](https://www.tpximpact.com/knowledge-hub/blogs/tech/hash-keys-data-warehousing-2/)\n",
    "- **meta_is_current**: Boolean of whether this row is the current (most recent).\n",
    "- **meta_valid_from**: Timestamp of when the row was first ingested.\n",
    "- **meta_valid_to**: Timestamp of then the row was outdated by a newer version (Null if row is current).\n",
    "- **meta_sequence**: Sequence number used for inserting data with duplicates of main key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    # Run and print SQL query\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "\n",
    "silver_table_schema(table_name=\"silver_sales_scd1\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_inventory_scd1\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "silver_table_schema(table_name=\"silver_product_scd2\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "silver_table_schema(table_name=\"silver_customer_scd2\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 1\n",
    "\n",
    "In the Type 1 SCD, you simply overwrite data in dimensions.\n",
    "\n",
    "[Read more here.](https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd1_table(df: DataFrame, id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    # TODO: Find a better way to pass input parameters to functions \n",
    "    target = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT *\n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_scd1_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_last_updated\", current_timestamp())\n",
    "        .withColumn(\"parameter_target\", lit(target_table))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd1_table)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_scd1_streaming_table (\n",
    "    source_table=\"bronze_sales\",\n",
    "    target_table=\"silver_sales_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    join_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\"\n",
    ")\n",
    "\n",
    "silver_scd1_streaming_table (\n",
    "    source_table=\"bronze_inventory\",\n",
    "    target_table=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    join_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD) Type 2\n",
    "\n",
    "Type 2 Slowly Changing Dimensions in Data warehouse is the most popular dimension that is used in the data warehouse. As we discussed data warehouse is used for data analysis. If you need to analyze data, you need to accommodate historical aspects of data. Let us see how we can implement SCD Type 2.\n",
    "\n",
    "[Read more here.](https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd2_table(df: DataFrame, batch_id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    target_table = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "\n",
    "    # Drop extra columns\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Reorder dataframe to have same order as target table (otherwise insert statement might fail)\n",
    "    df_target = spark.read.table(target_table).limit(1)\n",
    "    df = df.select(df_target.columns)\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target_table} AS t\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(merge_query)\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target_table}\n",
    "            SELECT s.*\n",
    "            FROM tempView s\n",
    "            JOIN {target_table} t ON t.{join_key} = s.{join_key}\n",
    "            WHERE s.meta_sequence = {seq_num}\n",
    "            AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_scd2_streaming_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "\n",
    "    # TODO: Find a better way to pass arguments to the foreachBatch function!!!\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source_table)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "        .withColumn(\"parameter_target\", lit(target_table))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd2_table)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(target_table)\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SCD2 tables\n",
    "silver_scd2_streaming_table(\n",
    "    source_table = \"bronze_product\",\n",
    "    target_table = \"silver_product_scd2\",\n",
    "    join_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\"\n",
    ")\n",
    "\n",
    "silver_scd2_streaming_table(\n",
    "    source_table = \"bronze_customer\",\n",
    "    target_table = \"silver_customer_scd2\",\n",
    "    join_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Facts and Dimensions\n",
    "## What is Dimensional Modeling\n",
    "The data model used to store data in the denormalized form is called Dimensional Modeling. It is the technique of storing data in a Data Warehouse in such a way that enables fast query performance and easy access to its business users. It involves creating a set of dimensional tables that are designed to support business intelligence and reporting needs.\n",
    "\n",
    "The core concept of dimensional modeling is the creation of a star schema. It is called so as the tables are arranged in the form of a star.\n",
    "\n",
    "![alt text](images/star_schema.png \"Star Schema\")\n",
    "\n",
    "Dimensional modeling includes facts and dimensions. Let’s have a basic idea of what Facts and Dimensions are.\n",
    "\n",
    "## Fact Tables\n",
    "Fact tables are the heart of a data warehouse. They contain quantitative data, often referred to as measures or metrics, and are the focus of most data analysis. These tables store data related to business transactions and events, such as sales figures, revenue, or quantities sold. In essence, fact tables provide the “what” in data analysis.\n",
    "\n",
    "## Dimension Tables\n",
    "Dimension tables, on the other hand, offer context to the data stored in fact tables. They provide descriptive information that helps users understand the “who,” “where,” and “when” aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict,\n",
    "        include_unknown_column : bool\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE OR REPLACE TABLE {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "    if include_unknown_column:\n",
    "        print(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n",
    "        spark.sql(f\"INSERT INTO {table_name} ({surrogate_key}) VALUES ('N/A')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_product\",\n",
    "    source_table=\"silver_product_scd2\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_dim_customer\",\n",
    "    source_table=\"silver_customer_scd2\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    dim_table_refs=[],\n",
    "    include_unknown_column=True\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    include_unknown_column=False\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    include_unknown_column=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimension_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Print\n",
    "    print(\"\\nMerging into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    query = f\"SELECT * FROM {source_table}\"\n",
    "\n",
    "    # Add delta load logic if the target_table table already exists\n",
    "    if delta_load_column:\n",
    "        query += f\"\\n WHERE {delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target_table})\"\n",
    "\n",
    "    # Run query\n",
    "    df = spark.sql(query)\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dimension_table (\n",
    "    source_table=\"silver_product_scd2\",\n",
    "    target_table=\"gold_dim_product\",\n",
    "    surrogate_key=\"product_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n",
    "\n",
    "process_dimension_table (\n",
    "    source_table=\"silver_customer_scd2\",\n",
    "    target_table=\"gold_dim_customer\",\n",
    "    surrogate_key=\"customer_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dim_table_references(source, target, timestamp_key, dim_table_refs, delta_load_column, print_output=True):\n",
    "    \n",
    "    query_first = \"SELECT s.*\"\n",
    "    query_last = f\"\\nFROM {source} s\"\n",
    "\n",
    "    for ref in dim_table_refs:\n",
    "        \n",
    "        # Construct first part of query: Selects\n",
    "        query_first += f\"\"\", COALESCE({ref[\"table_name\"]}.{ref[\"surrogate_key\"]}, 'N/A') AS {ref[\"surrogate_key\"]} \"\"\"\n",
    "\n",
    "        # Construct last part of query: Joins\n",
    "        query_last += f\"\"\"\\nLEFT JOIN {ref[\"table_name\"]} ON {ref[\"table_name\"]}.{ref[\"join_key\"]} = s.{ref[\"join_key\"]}\n",
    "        AND s.{timestamp_key} BETWEEN {ref[\"table_name\"]}.meta_valid_from AND {ref[\"table_name\"]}.meta_valid_to\"\"\"\n",
    "\n",
    "    # Add delta load logic if the target table already exists\n",
    "    if delta_load_column:\n",
    "        query_last += f\"\\n WHERE s.{delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target})\"\n",
    "    \n",
    "    # Print output\n",
    "    if print_output:\n",
    "        print(query_first + query_last)\n",
    "\n",
    "    return query_first + query_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fact_table(\n",
    "    source_table : str, \n",
    "    target_table : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Print\n",
    "    print(\"\\nMerging into\", target_table)\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source_table,\n",
    "                                                 target=target_table,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target_table} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_fact_table (\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    target_table=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"gold_dim_customer\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "process_fact_table (\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    target_table=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"gold_dim_product\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
