{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"STREAMING_DWH\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database schemas - Bronze, Silver, Gold\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver;\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold;\")\n",
    "spark.sql(\"USE SCHEMA bronze;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def raw_ingestion(schema_name, table_name):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .load(f\"data/{table_name}\")\n",
    "        .withColumn(\"meta_timestamp\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{schema_name}.{table_name}\")\n",
    "        .toTable(f\"{schema_name}.{table_name}\")\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = raw_ingestion(schema_name=\"bronze\", table_name=\"inventory\")\n",
    "query2 = raw_ingestion(schema_name=\"bronze\", table_name=\"product\")\n",
    "query3 = raw_ingestion(schema_name=\"bronze\", table_name=\"purchase\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM bronze.product\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT current_schema()\").show()\n",
    "spark.sql(\"DESCRIBE SCHEMA EXTENDED bronze;\").show()\n",
    "spark.sql(\"SHOW TABLES IN bronze;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze.product\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLOWLY CHANGING DIMENSIONS (SCD) - TYPE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_table = \"tempView\"\n",
    "target_table = \"bronze.product_scd2\"\n",
    "merge_key = \"product_id\"\n",
    "timestamp = \"event_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Get list of all columns that doesn't contain \"meta_\"\n",
    "df = spark.sql(\"select * from bronze.product\")\n",
    "df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "df = df.withColumn(\"meta_valid_from\", df[timestamp])\n",
    "df = df.withColumn(\"meta_valid_to\", lit(None).cast(TimestampType()))\n",
    "\n",
    "# Create an empty Delta table with the same schema\n",
    "df.createOrReplaceTempView(\"tempView\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS bronze.product_scd2 LIKE tempView USING DELTA\")\n",
    "\n",
    "# Add partition column\n",
    "window_spec  = Window.partitionBy(\"product_id\").orderBy(\"event_time\")\n",
    "df = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE bronze.product_scd2\").show()\n",
    "spark.sql(\"SHOW COLUMNS FROM bronze.product_scd2\").show()\n",
    "spark.sql(\"select * from bronze.product_scd2\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_partitions = [i for i in df.select('row_number').orderBy(\"row_number\").distinct().collect()]\n",
    "\n",
    "for row_num in [1, 2]:\n",
    "\n",
    "    merge_query = \"\"\"\n",
    "        MERGE INTO {target_table} AS target\n",
    "        USING (\n",
    "            SELECT * FROM {source_table}\n",
    "            WHERE row_number = {partition}\n",
    "        ) AS source ON target.{merge_key} = source.{merge_key}\n",
    "        WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff\n",
    "        THEN UPDATE SET meta_is_current = false, meta_valid_to = source.{timestamp}\n",
    "        WHEN NOT MATCHED BY target THEN INSERT *\n",
    "    \"\"\".format(\n",
    "        source_table=source_table,\n",
    "        target_table=target_table,\n",
    "        merge_key=merge_key,\n",
    "        timestamp=timestamp,\n",
    "        partition=row_num\n",
    "    )\n",
    "\n",
    "    print(merge_query)\n",
    "    spark.sql(merge_query).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze.product_scd2 where meta_is_current = true\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "merge_query = \"\"\"\n",
    "  MERGE INTO {target_table} AS target\n",
    "  USING {source_table} AS source ON target.{merge_key} = source.{merge_key}\n",
    "  WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff \n",
    "  THEN UPDATE SET meta_is_current = false, meta_valid_to = source.{timestamp}\n",
    "  WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\".format(\n",
    "    source_table=source_table,\n",
    "    target_table=target_table,\n",
    "    merge_key=merge_key,\n",
    "    timestamp=timestamp\n",
    "  )\n",
    "\n",
    "spark.sql(merge_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze.product_scd2\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
