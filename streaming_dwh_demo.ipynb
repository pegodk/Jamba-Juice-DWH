{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"STREAMING_DWH\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- cogs: double (nullable = true)\n",
      " |-- contains_caffeine: boolean (nullable = true)\n",
      " |-- contains_fruit: boolean (nullable = true)\n",
      " |-- contains_nuts: boolean (nullable = true)\n",
      " |-- contains_veggies: boolean (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def create_bronze_streaming_table(source, target):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source)\n",
    "        .withColumn(\"meta_ingestion_ts\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target}\")\n",
    "        .toTable(target)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "query3 = create_bronze_streaming_table(source=\"data/purchase\", target=\"bronze_purchase\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+------------------+-----+----------+------+--------------------+--------------------+\n",
      "|            category|cogs|contains_caffeine|contains_fruit|contains_nuts|contains_veggies|          event_time|              item|price|product_id|  size|   meta_ingestion_ts|       meta_filename|\n",
      "+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+------------------+-----+----------+------+--------------------+--------------------+\n",
      "|Supercharged Smoo...| 2.7|            false|          true|        false|           false|2024-02-21 19:24:...|  Triple Berry Oat| 5.99|      SC01|24 oz.|2024-02-21 19:58:...|file:///home/pete...|\n",
      "|Supercharged Smoo...| 2.7|            false|         false|        false|           false|2024-02-21 19:24:...|   Peanut Paradise| 5.99|      SC02|24 oz.|2024-02-21 19:58:...|file:///home/pete...|\n",
      "|   Classic Smoothies| 1.5|            false|          true|        false|           false|2024-02-17 19:27:...|   Peaches â€˜n Silk| 4.99|      CS10|24 oz.|2024-02-21 19:58:...|file:///home/pete...|\n",
      "|Superfoods Smoothies| 2.1|            false|          true|        false|           false|2024-02-17 19:27:...| Caribbean C-Burst| 5.99|      SF05|24 oz.|2024-02-21 19:58:...|file:///home/pete...|\n",
      "|Superfoods Smoothies| 2.1|            false|          true|        false|            true|2024-02-17 19:27:...|Detox Island Green| 5.99|      SF07|24 oz.|2024-02-21 19:58:...|file:///home/pete...|\n",
      "+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+------------------+-----+----------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM bronze_product\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SILVER TABLES: SLOWLY CHANGING DIMENSIONS (SCD) - TYPE 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from utils import reorder_columns_in_dataframe\n",
    "\n",
    "def create_silver_scd1_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    timestamp_key : str,\n",
    "    merge_key: str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str = None\n",
    "):\n",
    "    \n",
    "    # Perform delta load or not?\n",
    "    if delta_load_column:\n",
    "        df = spark.sql(f\"select * from {source} where {delta_load_column} > (select max({delta_load_column}) from {target})\")\n",
    "        offset = spark.sql(f\"select max({surrogate_key}) as offset from {target}\").collect()[0].offset + 1\n",
    "    else:\n",
    "        spark.sql(f\"drop table if exists {target}\")\n",
    "        df = spark.sql(f\"select * from {source}\")\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta_last_updated\n",
    "    df = df.withColumn(\"meta_last_updated\", current_timestamp())\n",
    "\n",
    "    # Generate surrogate key\n",
    "    df = df.withColumn(surrogate_key, monotonically_increasing_id() + offset)\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "\n",
    "    # Reorder columns\n",
    "    df = reorder_columns_in_dataframe(df=df, \n",
    "                                      columns_to_front=[surrogate_key],\n",
    "                                      columns_to_back=[c for c in df.columns if \"meta_\" in c],\n",
    "                                      columns_to_delete=[\"meta_filename\"])\n",
    "    \n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {target} LIKE {tmp_view_name} USING DELTA\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 1 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS target\n",
    "            USING (\n",
    "                SELECT * FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS source ON target.{surrogate_key} = source.{surrogate_key}\n",
    "            WHEN MATCHED AND target.meta_hashdiff <> source.meta_hashdiff\n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_purchase_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              120|               0|               0|              120|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_purchase_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|               41|               0|               0|               41|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_purchase_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|               10|               0|               0|               10|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "Inserting into SILVER SCD TYPE 1 TABLE: silver_inventory_scd1\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|               45|               0|               0|               45|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_purchase\",\n",
    "    target=\"silver_purchase_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    merge_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    ")\n",
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_inventory\",\n",
    "    target=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    merge_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_scd2_table(\n",
    "    source: str, \n",
    "    target: str, \n",
    "    merge_key: str, \n",
    "    timestamp_key: str, \n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str = None\n",
    "):\n",
    "    \n",
    "    # Perform delta load or not?\n",
    "    if delta_load_column:\n",
    "        df = spark.sql(f\"select * from {source} where {delta_load_column} > (select max({delta_load_column}) from {target})\")\n",
    "        offset = spark.sql(f\"select max({surrogate_key}) as offset from {target}\").collect()[0].offset + 1\n",
    "    else:\n",
    "        spark.sql(f\"drop table if exists {target}\")\n",
    "        df = spark.sql(f\"select * from {source}\")\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate surrogate key\n",
    "    df = df.withColumn(surrogate_key, monotonically_increasing_id() + offset)\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "\n",
    "    # Reorder columns in dataframe\n",
    "    df = reorder_columns_in_dataframe(\n",
    "        df=df, \n",
    "        columns_to_front=[surrogate_key, merge_key],\n",
    "        columns_to_back=[c for c in df.columns if \"meta_\" in c],\n",
    "        columns_to_delete=[\"meta_filename\"]\n",
    "    )\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {target} LIKE {tmp_view_name} USING DELTA\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 2 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS target\n",
    "            USING (\n",
    "                SELECT * FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS source ON target.{merge_key} = source.{merge_key}\n",
    "            WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = source.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target}\n",
    "            SELECT * FROM \n",
    "            (\n",
    "                SELECT source.* \n",
    "                FROM {tmp_view_name} source\n",
    "                JOIN {target} target ON target.{merge_key} = source.{merge_key}\n",
    "                WHERE source.meta_sequence = {seq_num}\n",
    "                AND target.meta_hashdiff <> source.meta_hashdiff \n",
    "            )\n",
    "        \"\"\"\n",
    "        spark.sql(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SCD2 tables\n",
    "create_silver_scd2_table(\n",
    "    source = \"bronze_product\",\n",
    "    target = \"silver_product_scd2\",\n",
    "    merge_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------+---------------+-----+----------+--------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|transaction_sid|add_supplements|is_member|member_discount|price|product_id|quantity|supplement_price|total_purchase|     transaction_id|    transaction_time|   meta_ingestion_ts|       meta_hashdiff|   meta_last_updated|meta_sequence|\n",
      "+---------------+---------------+---------+---------------+-----+----------+--------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|    60129542148|          false|    false|            0.0| 5.99|      SF03|       1|             0.0|          5.99|1536494145524224281|2024-02-17 19:32:...|2024-02-21 19:58:...|6328ec5fc47ea0ba6...|2024-02-21 20:08:...|            3|\n",
      "|    51539607557|          false|     true|            0.1| 5.99|      SC05|       1|             0.0|          5.39|1554493886257160615|2024-02-18 19:27:...|2024-02-21 19:58:...|925645d026761d13a...|2024-02-21 20:08:...|            3|\n",
      "|    42949672970|          false|    false|            0.0| 4.99|      CS08|       2|             0.0|          9.98|1944245770584083946|2024-02-17 19:28:...|2024-02-21 19:58:...|37c21420083e1cfbf...|2024-02-21 20:08:...|            3|\n",
      "|    34359738389|          false|    false|            0.0| 5.49|      IS03|       1|             0.0|          5.49|2858393449141190714|2024-02-21 19:25:...|2024-02-21 19:58:...|24feaa91399717dfb...|2024-02-21 20:08:...|            3|\n",
      "|    60129542159|          false|    false|            0.0| 4.99|      CS09|       1|             0.0|          4.99|3843411071104270264|2024-02-17 19:33:...|2024-02-21 19:58:...|301ef8054fd9a7b9d...|2024-02-21 20:08:...|            3|\n",
      "+---------------+---------------+---------+---------------+-----+----------+--------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|inventory_sid|          event_time|existing_level|new_level|product_id|stock_quantity|   meta_ingestion_ts|       meta_hashdiff|   meta_last_updated|meta_sequence|\n",
      "+-------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|  42949672960|2024-02-17 19:27:...|            34|       44|      SC01|            10|2024-02-21 19:58:...|b193b27f72a8685a4...|2024-02-21 20:08:...|            1|\n",
      "|  42949672961|2024-02-17 19:27:...|            49|       59|      SF06|            10|2024-02-21 19:58:...|af9f74edf7247d629...|2024-02-21 20:08:...|            1|\n",
      "|  42949672962|2024-02-17 19:27:...|            48|       58|      SF04|            10|2024-02-21 19:58:...|d079350b537000225...|2024-02-21 20:08:...|            1|\n",
      "|  42949672963|2024-02-17 19:27:...|            34|       44|      SC03|            10|2024-02-21 19:58:...|3e82e56264b0286a1...|2024-02-21 20:08:...|            1|\n",
      "|  42949672964|2024-02-17 19:28:...|            48|       58|      SF05|            10|2024-02-21 19:58:...|21b50d574f833ce25...|2024-02-21 20:08:...|            1|\n",
      "+-------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+----------+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+---------------+-----+------+--------------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "|product_sid|product_id|            category|cogs|contains_caffeine|contains_fruit|contains_nuts|contains_veggies|          event_time|           item|price|  size|   meta_ingestion_ts|       meta_hashdiff|meta_is_current|     meta_valid_from|       meta_valid_to|meta_sequence|\n",
      "+-----------+----------+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+---------------+-----+------+--------------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "|          0|      CS07|   Classic Smoothies| 1.5|            false|          true|        false|           false|2024-02-18 19:26:...|Blueberry Bliss| 4.99|24 oz.|2024-02-21 19:58:...|15cc375354ec3db62...|          false|2024-02-18 19:26:...|2024-02-21 19:24:...|            3|\n",
      "|          0|      CS07|   Classic Smoothies| 1.5|            false|          true|        false|           false|2024-02-18 19:26:...|Blueberry Bliss| 4.99|24 oz.|2024-02-21 19:58:...|15cc375354ec3db62...|          false|2024-02-18 19:26:...|2024-02-21 19:24:...|            3|\n",
      "|          1|      SF02|Superfoods Smoothies| 2.1|            false|          true|        false|            true|2024-02-18 19:26:...|  Totally Green| 5.99|24 oz.|2024-02-21 19:58:...|44cf7ea4a75ebbc56...|          false|2024-02-18 19:26:...|2024-02-21 19:24:...|            3|\n",
      "|          1|      SF02|Superfoods Smoothies| 2.1|            false|          true|        false|            true|2024-02-18 19:26:...|  Totally Green| 5.99|24 oz.|2024-02-21 19:58:...|44cf7ea4a75ebbc56...|          false|2024-02-18 19:26:...|2024-02-21 19:24:...|            3|\n",
      "|          2|      SC03|Supercharged Smoo...| 2.7|            false|         false|         true|           false|2024-02-18 19:26:...|     Health Nut| 5.99|24 oz.|2024-02-21 19:58:...|120309dc5be633e6a...|          false|2024-02-18 19:26:...|2024-02-21 19:24:...|            3|\n",
      "+-----------+----------+--------------------+----+-----------------+--------------+-------------+----------------+--------------------+---------------+-----+------+--------------------+--------------------+---------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from silver_purchase_scd1\").show(5)\n",
    "spark.sql(\"select * from silver_inventory_scd1\").show(5)\n",
    "spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_gold_dimension_table():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_dim_table_references\n",
    "\n",
    "def create_gold_fact_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str = None\n",
    "):\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source,\n",
    "                                                 target=target,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Reorder columns in dataframe\n",
    "    df = reorder_columns_in_dataframe(\n",
    "        df=df, \n",
    "        columns_to_front=[surrogate_key] + [r[\"surrogate_key\"] for r in dim_table_refs],\n",
    "        columns_to_back=[c for c in df.columns if \"meta_\" in c]\n",
    "    )\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {target} LIKE {tmp_view_name} USING DELTA\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target} AS target\n",
    "        USING {tmp_view_name} AS source ON target.{surrogate_key} = source.{surrogate_key}\n",
    "        WHEN MATCHED AND target.meta_hashdiff <> source.meta_hashdiff THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT src.*, silver_product_scd2.product_sid \n",
      "FROM silver_purchase_scd1 src\n",
      "LEFT JOIN silver_product_scd2 ON silver_product_scd2.product_id = src.product_id\n",
      "        AND src.transaction_time BETWEEN silver_product_scd2.meta_valid_from AND silver_product_scd2.meta_valid_to\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|              292|               0|               0|              292|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "SELECT src.*, silver_product_scd2.product_sid \n",
      "FROM silver_inventory_scd1 src\n",
      "LEFT JOIN silver_product_scd2 ON silver_product_scd2.product_id = src.product_id\n",
      "        AND src.event_time BETWEEN silver_product_scd2.meta_valid_from AND silver_product_scd2.meta_valid_to\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|               82|               0|               0|               82|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_gold_fact_table (\n",
    "    source=\"silver_purchase_scd1\",\n",
    "    target=\"gold_fact_purchase\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")\n",
    "\n",
    "create_gold_fact_table (\n",
    "    source=\"silver_inventory_scd1\",\n",
    "    target=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|inventory_sid|product_sid|          event_time|existing_level|new_level|product_id|stock_quantity|   meta_ingestion_ts|       meta_hashdiff|   meta_last_updated|meta_sequence|\n",
      "+-------------+-----------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "|  42949672960|42949672965|2024-02-17 19:27:...|            34|       44|      SC01|            10|2024-02-21 19:58:...|b193b27f72a8685a4...|2024-02-21 20:08:...|            1|\n",
      "|  42949672961| 8589934594|2024-02-17 19:27:...|            49|       59|      SF06|            10|2024-02-21 19:58:...|af9f74edf7247d629...|2024-02-21 20:08:...|            1|\n",
      "|  42949672962|42949672964|2024-02-17 19:27:...|            48|       58|      SF04|            10|2024-02-21 19:58:...|d079350b537000225...|2024-02-21 20:08:...|            1|\n",
      "|  42949672963| 8589934600|2024-02-17 19:27:...|            34|       44|      SC03|            10|2024-02-21 19:58:...|3e82e56264b0286a1...|2024-02-21 20:08:...|            1|\n",
      "|  42949672964|34359738371|2024-02-17 19:28:...|            48|       58|      SF05|            10|2024-02-21 19:58:...|21b50d574f833ce25...|2024-02-21 20:08:...|            1|\n",
      "+-------------+-----------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"select * from gold_fact_purchase\").show(5)\n",
    "spark.sql(\"select * from gold_fact_inventory\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT src.*, silver_product_scd2.product_sid \n",
      "FROM silver_purchase_scd1 src\n",
      "LEFT JOIN silver_product_scd2 ON silver_product_scd2.product_id = src.product_id\n",
      "        AND src.transaction_time BETWEEN silver_product_scd2.meta_valid_from AND silver_product_scd2.meta_valid_to\n"
     ]
    },
    {
     "ename": "UnsupportedOperationException",
     "evalue": "[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m\n\u001b[1;32m     25\u001b[0m create_silver_scd2_table(\n\u001b[1;32m     26\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_product\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_product_scd2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     delta_load_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# GOLD\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mcreate_gold_fact_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilver_purchase_scd1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgold_fact_purchase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransaction_sid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransaction_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_table_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilver_product_scd2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerge_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurrogate_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_sid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m create_gold_fact_table (\n\u001b[1;32m     44\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_inventory_scd1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_fact_inventory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     dim_table_refs\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilver_product_scd2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerge_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogate_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_sid\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m     49\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36mcreate_gold_fact_table\u001b[0;34m(source, target, surrogate_key, timestamp_key, dim_table_refs, delta_load_column)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Merge into target table \u001b[39;00m\n\u001b[1;32m     33\u001b[0m merge_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m    MERGE INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AS target\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    USING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp_view_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AS source ON target.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msurrogate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = source.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msurrogate_key\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m    WHEN MATCHED AND target.meta_hashdiff <> source.meta_hashdiff THEN UPDATE SET *\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m    WHEN NOT MATCHED THEN INSERT *\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_query\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # BRONZE\n",
    "    query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "    query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "    query3 = create_bronze_streaming_table(source=\"data/purchase\", target=\"bronze_purchase\")\n",
    "\n",
    "    # SILVER\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_purchase\",\n",
    "        target=\"silver_purchase_scd1\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        merge_key=\"transaction_id\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        delta_load_column=\"transaction_time\"\n",
    "    )\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_inventory\",\n",
    "        target=\"silver_inventory_scd1\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        merge_key=\"event_time\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "    create_silver_scd2_table(\n",
    "        source = \"bronze_product\",\n",
    "        target = \"silver_product_scd2\",\n",
    "        merge_key = \"product_id\",\n",
    "        timestamp_key = \"event_time\",\n",
    "        surrogate_key = \"product_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "\n",
    "    # GOLD\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_purchase_scd1\",\n",
    "        target=\"gold_fact_purchase\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    "    )\n",
    "\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_inventory_scd1\",\n",
    "        target=\"gold_fact_inventory\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
