{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO STREAMING DATAWAREHOUSE\n",
    "\n",
    "![alt text](images/delta_dwh.png \"Data Warehouse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"JAMBA_JUICE\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- cogs: double (nullable = true)\n",
      " |-- contains_caffeine: boolean (nullable = true)\n",
      " |-- contains_fruit: boolean (nullable = true)\n",
      " |-- contains_nuts: boolean (nullable = true)\n",
      " |-- contains_veggies: boolean (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bronze Tables\n",
    "This function will create empty tables for future use.\n",
    "\n",
    "The following meta columns are added to the table:\n",
    "- meta_created\n",
    "- meta_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def create_bronze_streaming_table(source, target):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source)\n",
    "        .withColumn(\"meta_created\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .trigger(processingTime='10 seconds')\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target}\")\n",
    "        .toTable(target)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "bronze_query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "bronze_query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "bronze_query3 = create_bronze_streaming_table(source=\"data/sales\", target=\"bronze_sales\")\n",
    "bronze_query4 = create_bronze_streaming_table(source=\"data/customer\", target=\"bronze_customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM bronze_sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Silver Tables\n",
    "This function will create empty tables for future use.\n",
    "\n",
    "Depending on the slowly changing dimension (SCD) type, the following columns will be created:\n",
    "\n",
    "SCD type 1:\n",
    "- meta_hashdiff\n",
    "- meta_last_updated\n",
    "- meta_sequence\n",
    "\n",
    "SCD type 2:\n",
    "- meta_hashdiff\n",
    "- meta_is_current\n",
    "- meta_valid_from\n",
    "- meta_valid_to\n",
    "- meta_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS silver_sales_scd1 (transaction_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_inventory_scd1 (inventory_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_product_scd2 (product_sid string, category string, cogs double, contains_caffeine boolean, contains_fruit boolean, contains_nuts boolean, contains_veggies boolean, event_time string, item string, price double, product_id string, size string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n",
      "CREATE TABLE IF NOT EXISTS silver_customer_scd2 (customer_sid string, address string, credit_card_expire string, credit_card_number string, customer_id bigint, email string, event_time string, full_name string, phone_number string, meta_created timestamp, meta_filename string,meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\n"
     ]
    }
   ],
   "source": [
    "def create_silver_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : int\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA\"\n",
    "\n",
    "    # Run and print SQL query\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "\n",
    "create_silver_table_schema(table_name=\"silver_sales_scd1\", surrogate_key=\"transaction_sid\", source_table=\"bronze_sales\", scd_type=1)\n",
    "create_silver_table_schema(table_name=\"silver_inventory_scd1\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "create_silver_table_schema(table_name=\"silver_product_scd2\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n",
    "create_silver_table_schema(table_name=\"silver_customer_scd2\", surrogate_key=\"customer_sid\", source_table=\"bronze_customer\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|  transaction_sid|   string|   NULL|\n",
      "|      customer_id|   bigint|   NULL|\n",
      "|  member_discount|   double|   NULL|\n",
      "|            price|   double|   NULL|\n",
      "|       product_id|   string|   NULL|\n",
      "|         quantity|   bigint|   NULL|\n",
      "| supplement_price|   double|   NULL|\n",
      "|   total_purchase|   double|   NULL|\n",
      "|   transaction_id|   string|   NULL|\n",
      "| transaction_time|   string|   NULL|\n",
      "|     meta_created|timestamp|   NULL|\n",
      "|    meta_filename|   string|   NULL|\n",
      "|    meta_hashdiff|   string|   NULL|\n",
      "|meta_last_updated|timestamp|   NULL|\n",
      "|    meta_sequence|      int|   NULL|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table silver_sales_scd1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Tables: Slowly Changing Dimensions (SCD) Type 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd1_table(df: DataFrame, id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    # TODO: Find a better way to pass input parameters to functions \n",
    "    target = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT *\n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(query)\n",
    "\n",
    "\n",
    "def create_silver_streaming_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_last_updated\", current_timestamp())\n",
    "        .withColumn(\"parameter_target\", lit(target))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd1_table)\n",
    "        .outputMode(\"update\")\n",
    "        .start()\n",
    "    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_query1 = create_silver_streaming_table (\n",
    "    source=\"bronze_sales\",\n",
    "    target=\"silver_sales_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    join_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\"\n",
    ")\n",
    "\n",
    "silver_query2 = create_silver_streaming_table (\n",
    "    source=\"bronze_inventory\",\n",
    "    target=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    join_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|sales|\n",
      "+-----+\n",
      "|   13|\n",
      "+-----+\n",
      "\n",
      "+---------+\n",
      "|inventory|\n",
      "+---------+\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as sales from silver_sales_scd1\").show()\n",
    "\n",
    "spark.sql(\"select count(*) as inventory from silver_inventory_scd1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number, column\n",
    "from pyspark.sql.types import BooleanType, TimestampType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def upsert_to_scd2_table(df: DataFrame, id: int):\n",
    "    \n",
    "    # Get input parameters from columns\n",
    "    target = df.select(\"parameter_target\").limit(1).collect()[0][0]\n",
    "    timestamp_key = df.select(\"parameter_timestamp_key\").limit(1).collect()[0][0]\n",
    "    join_key = df.select(\"parameter_join_key\").limit(1).collect()[0][0]\n",
    "\n",
    "    # Drop extra columns\n",
    "    df = df.drop(\"parameter_target\")\n",
    "    df = df.drop(\"parameter_timestamp_key\")\n",
    "    df = df.drop(\"parameter_join_key\")\n",
    "\n",
    "    # Calculate hashdiff\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate sequence number\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(Window.partitionBy(join_key).orderBy(timestamp_key)))\n",
    "\n",
    "    # Reorder dataframe to have same order as target table (otherwise insert statement might fail)\n",
    "    df_target = spark.read.table(target).limit(1)\n",
    "    df = df.select(df_target.columns)\n",
    "\n",
    "    # Create view with source data\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS t\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM tempView\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS s ON t.{join_key} = s.{join_key}\n",
    "            WHEN MATCHED AND t.meta_is_current = true AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = s.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(merge_query)\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target}\n",
    "            SELECT s.*\n",
    "            FROM tempView s\n",
    "            JOIN {target} t ON t.{join_key} = s.{join_key}\n",
    "            WHERE s.meta_sequence = {seq_num}\n",
    "            AND t.meta_hashdiff <> s.meta_hashdiff\n",
    "        \"\"\"\n",
    "        df.sparkSession.sql(insert_query)\n",
    "\n",
    "\n",
    "def create_silver_scd2_streaming_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    timestamp_key : str,\n",
    "    join_key: str,\n",
    "    surrogate_key : str\n",
    "):\n",
    "\n",
    "    # TODO: Find a better way to pass arguments to the foreachBatch function!!!\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .table(source)\n",
    "        .withColumn(surrogate_key, md5(column(join_key).cast(BinaryType())))\n",
    "        .withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "        .withColumn(\"parameter_target\", lit(target))\n",
    "        .withColumn(\"parameter_timestamp_key\", lit(timestamp_key))\n",
    "        .withColumn(\"parameter_join_key\", lit(join_key))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .foreachBatch(upsert_to_scd2_table)\n",
    "        .outputMode(\"update\")\n",
    "        .start()\n",
    "    )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SCD2 tables\n",
    "silver_query3 = create_silver_scd2_streaming_table(\n",
    "    source = \"bronze_product\",\n",
    "    target = \"silver_product_scd2\",\n",
    "    join_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\"\n",
    ")\n",
    "\n",
    "silver_query4 = create_silver_scd2_streaming_table(\n",
    "    source = \"bronze_customer\",\n",
    "    target = \"silver_customer_scd2\",\n",
    "    join_key = \"customer_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"customer_sid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|products|\n",
      "+--------+\n",
      "|      27|\n",
      "+--------+\n",
      "\n",
      "+---------+\n",
      "|customers|\n",
      "+---------+\n",
      "|      119|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as products from silver_product_scd2\").show()\n",
    "spark.sql(\"select count(*) as customers from silver_customer_scd2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|CAST(hello AS BINARY)|\n",
      "+---------------------+\n",
      "|     [68 65 6C 6C 6F]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cast('hello' as binary)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+-----------+--------------------+--------------------+---------------+---------------+--------------------+--------------------+\n",
      "|             address|credit_card_expire|credit_card_number|customer_id|               email|          event_time|      full_name|   phone_number|        meta_created|       meta_filename|\n",
      "+--------------------+------------------+------------------+-----------+--------------------+--------------------+---------------+---------------+--------------------+--------------------+\n",
      "|35203 Smith Junct...|             10/24|     4221406063287|          1|heather26@example...|2024-04-17 16:47:...|  William Moore|   455.490.6008|2024-04-17 16:47:...|file:///home/pete...|\n",
      "|6386 Madison Road...|             10/31|  4749505413855012|          2|gdougherty@exampl...|2024-04-17 16:47:...|Jacqueline Boyd|+1-426-857-6347|2024-04-17 16:48:...|file:///home/pete...|\n",
      "|6386 Madison Road...|             10/31|  4749505413855012|          2|gdougherty@exampl...|2024-04-17 16:47:...|Jacqueline Boyd|+1-426-857-6347|2024-04-17 16:47:...|file:///home/pete...|\n",
      "|6386 Madison Road...|             10/31|  4749505413855012|          2|gdougherty@exampl...|2024-04-17 16:47:...|Jacqueline Boyd|+1-426-857-6347|2024-04-17 16:47:...|file:///home/pete...|\n",
      "+--------------------+------------------+------------------+-----------+--------------------+--------------------+---------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from bronze_customer order by customer_id\").show(5)\n",
    "\n",
    "# spark.sql(\"select * from silver_sales_scd1\").show(5)\n",
    "# spark.sql(\"select * from silver_inventory_scd1\").show(5)\n",
    "# spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(5)\n",
    "# spark.sql(\"select * from silver_customer_scd2 order by customer_sid\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE GOLD TABLES - FACTS AND DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gold_table_schema(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS gold_fact_sales (transaction_sid string, product_sid string, customer_id bigint, member_discount double, price double, product_id string, quantity bigint, supplement_price double, total_purchase double, transaction_id string, transaction_time string, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n",
      "CREATE TABLE IF NOT EXISTS gold_fact_inventory (inventory_sid string, product_sid string, event_time string, existing_level bigint, new_level bigint, product_id string, stock_quantity bigint, meta_created timestamp, meta_filename string, meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\n"
     ]
    }
   ],
   "source": [
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_sales\",\n",
    "    source_table=\"silver_sales_scd1\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")\n",
    "\n",
    "create_gold_table_schema (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dim_table_references(source, target, timestamp_key, dim_table_refs, delta_load_column, print_output=True):\n",
    "    \n",
    "    query_first = \"SELECT s.*\"\n",
    "    query_last = f\"\\nFROM {source} s\"\n",
    "\n",
    "    for ref in dim_table_refs:\n",
    "        \n",
    "        # Construct first part of query: Selects\n",
    "        query_first += f\"\"\", {ref[\"table_name\"]}.{ref[\"surrogate_key\"]} \"\"\"\n",
    "\n",
    "        # Construct last part of query: Joins\n",
    "        query_last += f\"\"\"\\nLEFT JOIN {ref[\"table_name\"]} ON {ref[\"table_name\"]}.{ref[\"join_key\"]} = s.{ref[\"join_key\"]}\n",
    "        AND s.{timestamp_key} BETWEEN {ref[\"table_name\"]}.meta_valid_from AND {ref[\"table_name\"]}.meta_valid_to\"\"\"\n",
    "\n",
    "    # Add delta load logic if the target table already exists\n",
    "    if delta_load_column:\n",
    "        query_last += f\"\\n WHERE s.{delta_load_column} > (SELECT COALESCE(MAX({delta_load_column}), '1970-01-01') FROM {target})\"\n",
    "    \n",
    "    # Print output\n",
    "    if print_output:\n",
    "        print(query_first + query_last)\n",
    "\n",
    "    return query_first + query_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_fact_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source,\n",
    "                                                 target=target,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    df.createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target} AS t\n",
    "        USING tempView AS s\n",
    "            ON t.{surrogate_key} = s.{surrogate_key}\n",
    "        WHEN MATCHED AND t.meta_hashdiff <> s.meta_hashdiff \n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED \n",
    "            THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT s.*, silver_product_scd2.product_sid , silver_customer_scd2.customer_sid \n",
      "FROM silver_sales_scd1 s\n",
      "LEFT JOIN silver_product_scd2 ON silver_product_scd2.product_id = s.product_id\n",
      "        AND s.transaction_time BETWEEN silver_product_scd2.meta_valid_from AND silver_product_scd2.meta_valid_to\n",
      "LEFT JOIN silver_customer_scd2 ON silver_customer_scd2.customer_id = s.customer_id\n",
      "        AND s.transaction_time BETWEEN silver_customer_scd2.meta_valid_from AND silver_customer_scd2.meta_valid_to\n",
      " WHERE s.transaction_time > (SELECT COALESCE(MAX(transaction_time), '1970-01-01') FROM gold_fact_sales)\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                7|               0|               0|                7|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n",
      "SELECT s.*, silver_product_scd2.product_sid \n",
      "FROM silver_inventory_scd1 s\n",
      "LEFT JOIN silver_product_scd2 ON silver_product_scd2.product_id = s.product_id\n",
      "        AND s.event_time BETWEEN silver_product_scd2.meta_valid_from AND silver_product_scd2.meta_valid_to\n",
      " WHERE s.event_time > (SELECT COALESCE(MAX(event_time), '1970-01-01') FROM gold_fact_inventory)\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                3|               0|               0|                3|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_gold_fact_table (\n",
    "    source=\"silver_sales_scd1\",\n",
    "    target=\"gold_fact_sales\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[\n",
    "        {\"table_name\": \"silver_product_scd2\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"},\n",
    "        {\"table_name\": \"silver_customer_scd2\", \"join_key\": \"customer_id\", \"surrogate_key\": \"customer_sid\"},\n",
    "    ],\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "create_gold_fact_table (\n",
    "    source=\"silver_inventory_scd1\",\n",
    "    target=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"join_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|       inventory_sid|         product_sid|          event_time|existing_level|new_level|product_id|stock_quantity|        meta_created|       meta_filename|       meta_hashdiff|   meta_last_updated|meta_sequence|\n",
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|6e5cf081b2e954385...|b19c2e29b9abe277f...|2024-04-17 16:47:...|            48|       58|      SF07|            10|2024-04-17 16:47:...|file:///home/pete...|caa1c16da83e5b267...|2024-04-17 16:47:...|            1|\n",
      "|1b4639fe3e1668279...|8d73dd95531fed18d...|2024-04-17 16:47:...|            33|       43|      SC02|            10|2024-04-17 16:48:...|file:///home/pete...|41389dbaaead1fe7d...|2024-04-17 16:48:...|            1|\n",
      "|a737bded6b57f1c71...|7530cfc1dc2f00234...|2024-04-17 16:48:...|            49|       59|      SF05|            10|2024-04-17 16:48:...|file:///home/pete...|818484489d05dc17b...|2024-04-17 16:48:...|            1|\n",
      "+--------------------+--------------------+--------------------+--------------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"select * from gold_fact_sales\").show(5)\n",
    "spark.sql(\"select * from gold_fact_inventory\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
