{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up folders (metastore and spark-warehouse)\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# for item in [\"metastore_db\", \"spark-warehouse\"]:\n",
    "#     if os.path.exists(item):\n",
    "#         shutil.rmtree(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"STREAMING_DWH\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "# Create the streaming_df to read from input directory\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"data/product/\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def create_bronze_streaming_table(source, target):\n",
    "\n",
    "    # Generates a source path based on table name, reads all files from that and inserts into bronze schema\n",
    "    query = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .load(source)\n",
    "        .withColumn(\"meta_ingestion_ts\", current_timestamp())\n",
    "        .withColumn(\"meta_filename\", input_file_name())\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", f\"spark-warehouse/_checkpoints/{target}\")\n",
    "        .toTable(target)\n",
    "    )\n",
    "    return query\n",
    "\n",
    "query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "query3 = create_bronze_streaming_table(source=\"data/purchase\", target=\"bronze_purchase\")\n",
    "\n",
    "# Use the code \n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE SILVER TABLES\n",
    "This function will create empty tables for future use.\n",
    "\n",
    "Depending on the SCD type, the following columns will be created:\n",
    "\n",
    "SCD type 1:\n",
    "- meta_hashdiff\n",
    "- meta_last_updated\n",
    "- meta_sequence\n",
    "\n",
    "SCD type 2:\n",
    "- meta_hashdiff\n",
    "- meta_is_current\n",
    "- meta_valid_from\n",
    "- meta_valid_to\n",
    "- meta_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_table(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        scd_type : str\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string,\"\n",
    "    \n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        query += f\" {row['col_name']} {row['data_type']},\"\n",
    "\n",
    "    # Add extra meta columns depending on SCD (slowly changing dimension) type\n",
    "    if scd_type == 1:\n",
    "        query += \"meta_hashdiff string, meta_last_updated timestamp, meta_sequence int) USING DELTA;\"\n",
    "    elif scd_type == 2:\n",
    "        query += \"meta_hashdiff string, meta_is_current boolean, meta_valid_from timestamp, meta_valid_to timestamp, meta_sequence int) USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "\n",
    "create_silver_table(table_name=\"silver_purchase_scd1\", surrogate_key=\"transaction_sid\", source_table=\"bronze_purchase\", scd_type=1)\n",
    "create_silver_table(table_name=\"silver_inventory_scd1\", surrogate_key=\"inventory_sid\", source_table=\"bronze_inventory\", scd_type=1)\n",
    "create_silver_table(table_name=\"silver_product_scd2\", surrogate_key=\"product_sid\", source_table=\"bronze_product\", scd_type=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from silver_product_scd2\").show(5)\n",
    "# spark.sql(f\"select * from silver_purchase_scd1\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SILVER TABLES: SLOWLY CHANGING DIMENSIONS (SCD) - TYPE 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, lit, row_number\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def create_silver_scd1_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    timestamp_key : str,\n",
    "    merge_key: str,\n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "    \n",
    "    # Perform delta load\n",
    "    df = spark.sql(f\"select * from {source} where {delta_load_column} > (select coalesce(max({delta_load_column}), '1970-01-01') from {target})\")\n",
    "\n",
    "    # Calculate surrogate key as hash of natural key columns\n",
    "    df = df.withColumn(surrogate_key, md5(merge_key))\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta_last_updated\n",
    "    df = df.withColumn(\"meta_last_updated\", current_timestamp())\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "    \n",
    "    # Create view with source data\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD1 table\n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 1 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS target\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS source ON target.{surrogate_key} = source.{surrogate_key}\n",
    "            WHEN MATCHED AND target.meta_hashdiff <> source.meta_hashdiff \n",
    "                THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_purchase\",\n",
    "    target=\"silver_purchase_scd1\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    merge_key=\"transaction_id\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "create_silver_scd1_table (\n",
    "    source=\"bronze_inventory\",\n",
    "    target=\"silver_inventory_scd1\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    merge_key=\"event_time\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_scd2_table(\n",
    "    source: str, \n",
    "    target: str, \n",
    "    merge_key: str, \n",
    "    timestamp_key: str, \n",
    "    surrogate_key : str,\n",
    "    delta_load_column: str\n",
    "):\n",
    "    \n",
    "    # Perform delta load\n",
    "    df = spark.sql(f\"select * from {source} where {delta_load_column} > (select coalesce(max({delta_load_column}), '1970-01-01') from {target})\")\n",
    "\n",
    "    # Calculate surrogate key as hash of natural key columns\n",
    "    df = df.withColumn(surrogate_key, md5(merge_key))\n",
    "\n",
    "    # Calculate hashdiff string based on all columns that doesn't contain \"meta_\" in the name\n",
    "    df = df.withColumn(\"meta_hashdiff\", md5(concat_ws(\"||\", *[c for c in df.columns if \"meta_\" not in c])))\n",
    "\n",
    "    # Set default values for meta columns\n",
    "    df = df.withColumn(\"meta_is_current\", lit(1).cast(BooleanType()))\n",
    "    df = df.withColumn(\"meta_valid_from\", df[timestamp_key])\n",
    "    df = df.withColumn(\"meta_valid_to\", lit('9999-12-31').cast(TimestampType()))\n",
    "\n",
    "    # Calculate sequence numbers if source data contain multiple rows for each merge_key\n",
    "    window_spec = Window.partitionBy(merge_key).orderBy(timestamp_key)\n",
    "    df = df.withColumn(\"meta_sequence\", row_number().over(window_spec))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "\n",
    "    # Get list of sequences\n",
    "    lst_sequence = sorted([p.meta_sequence for p in df.select('meta_sequence').distinct().collect()])\n",
    "\n",
    "    # Run SCD2 table \n",
    "    for seq_num in lst_sequence:\n",
    "        print(f\"Inserting into SILVER SCD TYPE 2 TABLE: {target}\")\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {target} AS target\n",
    "            USING (\n",
    "                SELECT * \n",
    "                FROM {tmp_view_name}\n",
    "                WHERE meta_sequence = {seq_num}\n",
    "            ) AS source ON target.{merge_key} = source.{merge_key}\n",
    "            WHEN MATCHED AND target.meta_is_current = true AND target.meta_hashdiff <> source.meta_hashdiff\n",
    "                THEN UPDATE SET meta_is_current = false, meta_valid_to = source.{timestamp_key}\n",
    "            WHEN NOT MATCHED \n",
    "                THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_query).show()\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {target}\n",
    "            SELECT * FROM \n",
    "            (\n",
    "                SELECT source.*\n",
    "                FROM {tmp_view_name} source\n",
    "                JOIN {target} target ON target.{merge_key} = source.{merge_key}\n",
    "                WHERE source.meta_sequence = {seq_num}\n",
    "                AND target.meta_hashdiff <> source.meta_hashdiff \n",
    "            )\n",
    "        \"\"\"\n",
    "        spark.sql(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SCD2 tables\n",
    "create_silver_scd2_table(\n",
    "    source = \"bronze_product\",\n",
    "    target = \"silver_product_scd2\",\n",
    "    merge_key = \"product_id\",\n",
    "    timestamp_key = \"event_time\",\n",
    "    surrogate_key = \"product_sid\",\n",
    "    delta_load_column=\"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from silver_purchase_scd1\").show(5)\n",
    "spark.sql(\"select * from silver_inventory_scd1\").show(5)\n",
    "spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE GOLD TABLES - FACTS AND DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gold_table(\n",
    "        table_name : str, \n",
    "        surrogate_key : str, \n",
    "        source_table : str, \n",
    "        dim_table_refs : dict\n",
    "    ):\n",
    "\n",
    "    # Define table name and surrogate key\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({surrogate_key} string\"\n",
    "\n",
    "    # Loop through and add surrogate keys for foreign keys\n",
    "    for row in dim_table_refs:\n",
    "        query += f\", {row['surrogate_key']} string\"\n",
    "\n",
    "    # Get schema of source table\n",
    "    source_schema = spark.sql(f\"describe table {source_table}\").collect()\n",
    "    for row in source_schema:\n",
    "        if row['col_name'] != surrogate_key:\n",
    "            query += f\", {row['col_name']} {row['data_type']}\"\n",
    "\n",
    "    query += \") USING DELTA;\"\n",
    "\n",
    "    print(query)\n",
    "    spark.sql(query)\n",
    "    return\n",
    "\n",
    "\n",
    "create_gold_table (\n",
    "    table_name=\"gold_fact_purchase\",\n",
    "    source_table=\"silver_purchase_scd1\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")\n",
    "\n",
    "create_gold_table (\n",
    "    table_name=\"gold_fact_inventory\",\n",
    "    source_table=\"silver_inventory_scd1\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_dim_table_references\n",
    "\n",
    "def create_gold_fact_table(\n",
    "    source : str, \n",
    "    target : str,\n",
    "    surrogate_key : str,\n",
    "    timestamp_key : str,\n",
    "    dim_table_refs : dict,\n",
    "    delta_load_column: str\n",
    "):\n",
    "\n",
    "    # Generate and run SQL query\n",
    "    df = spark.sql(generate_dim_table_references(source=source,\n",
    "                                                 target=target,\n",
    "                                                 timestamp_key=timestamp_key, \n",
    "                                                 dim_table_refs=dim_table_refs, \n",
    "                                                 delta_load_column=delta_load_column))\n",
    "\n",
    "    # Create an empty Delta table with the same schema\n",
    "    tmp_view_name = \"temporaryView\"\n",
    "    df.createOrReplaceTempView(tmp_view_name)\n",
    "\n",
    "    # Merge into target table \n",
    "    merge_query = f\"\"\"\n",
    "        MERGE INTO {target} AS target\n",
    "        USING {tmp_view_name} AS source ON target.{surrogate_key} = source.{surrogate_key}\n",
    "        WHEN MATCHED AND target.meta_hashdiff <> source.meta_hashdiff THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gold_fact_table (\n",
    "    source=\"silver_purchase_scd1\",\n",
    "    target=\"gold_fact_purchase\",\n",
    "    surrogate_key=\"transaction_sid\",\n",
    "    timestamp_key=\"transaction_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"transaction_time\"\n",
    ")\n",
    "\n",
    "create_gold_fact_table (\n",
    "    source=\"silver_inventory_scd1\",\n",
    "    target=\"gold_fact_inventory\",\n",
    "    surrogate_key=\"inventory_sid\",\n",
    "    timestamp_key=\"event_time\",\n",
    "    dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "    delta_load_column=\"event_time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"select * from gold_fact_purchase\").show(5)\n",
    "spark.sql(\"select * from gold_fact_inventory\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # BRONZE\n",
    "    query1 = create_bronze_streaming_table(source=\"data/inventory\", target=\"bronze_inventory\")\n",
    "    query2 = create_bronze_streaming_table(source=\"data/product\", target=\"bronze_product\")\n",
    "    query3 = create_bronze_streaming_table(source=\"data/purchase\", target=\"bronze_purchase\")\n",
    "\n",
    "    # SILVER\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_purchase\",\n",
    "        target=\"silver_purchase_scd1\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        merge_key=\"transaction_id\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        delta_load_column=\"transaction_time\"\n",
    "    )\n",
    "    create_silver_scd1_table (\n",
    "        source=\"bronze_inventory\",\n",
    "        target=\"silver_inventory_scd1\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        merge_key=\"event_time\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "    create_silver_scd2_table(\n",
    "        source = \"bronze_product\",\n",
    "        target = \"silver_product_scd2\",\n",
    "        merge_key = \"product_id\",\n",
    "        timestamp_key = \"event_time\",\n",
    "        surrogate_key = \"product_sid\",\n",
    "        delta_load_column=\"event_time\"\n",
    "    )\n",
    "\n",
    "    # GOLD\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_purchase_scd1\",\n",
    "        target=\"gold_fact_purchase\",\n",
    "        surrogate_key=\"transaction_sid\",\n",
    "        timestamp_key=\"transaction_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "        delta_load_column=\"transaction_time\"\n",
    "    )\n",
    "\n",
    "    create_gold_fact_table (\n",
    "        source=\"silver_inventory_scd1\",\n",
    "        target=\"gold_fact_inventory\",\n",
    "        surrogate_key=\"inventory_sid\",\n",
    "        timestamp_key=\"event_time\",\n",
    "        dim_table_refs=[{\"table_name\": \"silver_product_scd2\", \"merge_key\": \"product_id\", \"surrogate_key\": \"product_sid\"}],\n",
    "        delta_load_column=\"event_time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from silver_product_scd2 order by product_sid\").show(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
